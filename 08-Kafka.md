# KAFKA

## Kafka Fundamentals

**\#. What is Apache Kafka and what problems does it solve?**

Apache Kafka is a distributed event streaming platform for building real-time data pipelines and streaming applications. Originally developed by LinkedIn, now Apache open-source project. **Problems solved**: **1\) Real-time Data Processing** \- handle streams of events in real-time. **2\) High Throughput** \- millions of messages per second. **3\) Scalability** \- distributed architecture, scales horizontally. **4\) Durability** \- persists data to disk, replicated across brokers. **5\) Decoupling** \- producers and consumers independent. **6\) Event Sourcing** \- immutable event log. **Use cases**: messaging, website activity tracking, metrics aggregation, log aggregation, stream processing, event sourcing, microservices communication. **Key features**: high throughput, scalability, fault-tolerance, durability, real-time processing. **Difference from traditional messaging**: not just message broker \- distributed commit log with persistence, replay capability, stream processing. Companies using: LinkedIn, Netflix, Uber, Airbnb for mission-critical data infrastructure.

**\#. Explain the core components of Kafka architecture.**

Kafka architecture has several key components: **1\) Broker** \- Kafka server that stores data, serves clients. Cluster has multiple brokers for scalability/fault-tolerance. **2\) Topic** \- category/feed of messages. Logical channel. E.g., "orders", "payments". **3\) Partition** \- topics split into partitions for parallelism. Each partition is ordered, immutable sequence of records. **4\) Producer** \- publishes messages to topics. **5\) Consumer** \- reads messages from topics. **6\) Consumer Group** \- group of consumers that work together, each partition consumed by one consumer in group. **7\) ZooKeeper** (legacy) \- coordinates cluster (broker metadata, leader election). Being removed in newer versions (KRaft mode). **8\) Offset** \- unique ID of message within partition, sequential counter. **Architecture**: producers write to partition leaders, data replicated to follower replicas, consumers read from leaders (or followers in newer versions), ZooKeeper manages cluster state. Partitions enable parallelism, replication enables fault-tolerance. This distributed architecture enables Kafka's scalability and reliability.

**\#. What is a Kafka topic and partition?**

**Topic**: logical category of messages. Like database table or messaging queue. Publishers send to topics, subscribers read from topics. Example topics: "user-registrations", "orders", "payment-events". Topics are multi-subscriber \- multiple consumers can read same data. **Partition**: topics divided into partitions for scalability and parallelism. Each partition is ordered, immutable log of records. Messages in partition have unique sequential ID called offset. **Why partition?**: **1\) Scalability** \- distribute data across brokers, store more than one server can hold. **2\) Parallelism** \- multiple consumers read different partitions simultaneously. **3\) Ordering** \- order guaranteed within partition, not across partitions. **Example**: "orders" topic with 10 partitions. Messages distributed across partitions. Each partition on different broker. 10 consumers can read in parallel, one per partition. **Partition key**: producers can specify key, messages with same key go to same partition (for ordering). No key → round-robin distribution. Understanding topics and partitions fundamental to Kafka design.

**\#. Explain Kafka producers and how they work.**

Producers publish messages to Kafka topics. **Process**: **1\) Create Producer** \- configure broker addresses, serializers. **2\) Create Record** \- ProducerRecord with topic, optional partition/key, value. **3\) Send** \- producer.send(record). **4\) Partitioner** \- determines which partition (based on key or round-robin). **5\) Batching** \- messages batched for efficiency. **6\) Send to Broker** \- batch sent to partition leader. **7\) Acknowledgment** \- broker acknowledges. **Configuration**: **bootstrap.servers** \- broker addresses. **key.serializer, value.serializer** \- convert objects to bytes. **acks** \- acknowledgment level (0=no ack, 1=leader ack, all=all replicas ack). **retries** \- retry on failure. **batch.size** \- batch size in bytes. **linger.ms** \- wait time before sending batch. **Partitioning strategies**: **1\) Key-based** \- hash(key) % partitions, same key → same partition (ordering). **2\) Round-robin** \- if no key, distribute evenly. **3\) Custom** \- implement Partitioner interface. **Example**: producer.send(new ProducerRecord\<\>("orders", orderId, order)). Producers handle batching, compression, retries automatically.

**\#. Explain Kafka consumers and consumer groups.**

Consumers read messages from topics. **Consumer Group**: multiple consumers working together as a group. Each partition consumed by exactly one consumer in group. Enables parallelism and load balancing. **How it works**: **1\) Subscribe** \- consumer subscribes to topics. **2\) Partition Assignment** \- Kafka assigns partitions to consumers in group. **3\) Rebalancing** \- when consumer joins/leaves, partitions reassigned. **4\) Poll** \- consumer polls for messages. **5\) Process** \- consumer processes messages. **6\) Commit Offset** \- consumer commits offset (acknowledging consumption). **Key points**: **1\) One partition → one consumer** in a group (can't have two consumers in same group reading same partition). **2\) Multiple groups** \- different groups can read same data independently. **3\) Scaling** \- add consumers to group (up to number of partitions). **Example**: "orders" topic, 10 partitions, consumer group with 5 consumers → each consumer gets 2 partitions. Add 5 more consumers → each gets 1 partition. Add more than 10 → some idle. **Multiple groups**: group A processes orders, group B for analytics, group C for email \- all read same "orders" data independently.

**\#. What are Kafka offsets and how are they managed?**

Offset: unique sequential ID assigned to each message within partition. Starts at 0, increments for each message. **Consumer offset**: position of consumer in partition \- which message it last read. **Offset management**: **1\) Commit** \- consumer tells Kafka "I've processed up to offset X". **2\) Storage** \- offsets stored in special topic "\_\_consumer\_offsets". **3\) Auto-commit** \- automatic periodic commit (enable.auto.commit=true, auto.commit.interval.ms=5000). **4\) Manual commit** \- explicit commit after processing (commitSync(), commitAsync()). **At-Least-Once**: commit after processing → message processed at least once (redelivery on failure). **At-Most-Once**: commit before processing → message lost on failure. **Exactly-Once**: complex, requires idempotent producer \+ transactional reads. **Seeking**: can reset offset \- seekToBeginning(), seekToEnd(), seek(partition, offset). **Resetting**: kafka-consumer-groups \--reset-offsets. **Best practice**: manual commit after successful processing for at-least-once guarantee. Idempotent processing handles duplicates. Offset tracking enables consumer to resume from where it left off.

**\#. What is the difference between Kafka and traditional message queues?**

**Traditional Message Queue (RabbitMQ, ActiveMQ)**: **1\) Message deletion** \- consumed messages deleted. **2\) Order** \- FIFO within queue. **3\) Throughput** \- moderate (thousands/sec). **4\) Use case** \- task distribution, async processing. **Kafka**: **1\) Retention** \- messages retained (configurable, days/size), can replay. **2\) Partitioning** \- order within partition, parallel across partitions. **3\) Throughput** \- very high (millions/sec). **4\) Use case** \- event streaming, log aggregation, real-time analytics. **Key differences**: **Persistence**: Queue deletes after consumption, Kafka retains (event log). **Multiple consumers**: Queue \- one consumer gets message, Kafka \- multiple groups can read. **Replay**: Queue \- no replay, Kafka \- replay from any offset. **Ordering**: Queue \- global order, Kafka \- per partition. **Performance**: Kafka much higher throughput. **Use Queue when**: simple pub/sub, task distribution, once-per-message consumption. **Use Kafka when**: high throughput, event sourcing, multiple consumers, replay needed, stream processing. Kafka is event streaming platform, not just message queue.

**\#. Explain Kafka replication and fault tolerance.**

Replication ensures data durability and availability. **Concepts**: **1\) Replication Factor** \- number of copies of partition (e.g., 3 means 3 copies). **2\) Leader** \- one replica is leader, handles reads/writes. **3\) Followers** \- other replicas, replicate data from leader. **4\) In-Sync Replicas (ISR)** \- replicas caught up with leader. **How it works**: **1\) Producer writes** to partition leader. **2\) Followers fetch** data from leader. **3\) Replication** \- followers replicate, join ISR when caught up. **4\) Leader failure** \- Kafka elects new leader from ISR. **5\) Recovery** \- failed broker rejoins, syncs data, becomes follower. **acks parameter**: **acks=0** \- no wait (fast, can lose data), **acks=1** \- wait for leader (balance), **acks=all** \- wait for all ISR (safe, slower). **min.insync.replicas**: minimum ISR for write to succeed (e.g., 2 means at least 2 replicas must acknowledge). **Example**: replication factor=3, min.insync.replicas=2. Even if 1 broker fails, writes succeed (2 remaining). Replication provides fault tolerance \- Kafka can survive broker failures without data loss.

**\#. What is ZooKeeper's role in Kafka and what is KRaft?**

**ZooKeeper** (traditional): distributed coordination service used by Kafka for: **1\) Broker Management** \- tracks which brokers are alive. **2\) Cluster Metadata** \- topic configurations, partitions, replicas. **3\) Leader Election** \- elects partition leaders. **4\) Configuration** \- stores cluster config. **Problems with ZooKeeper**: **1\) Operational complexity** \- separate system to manage. **2\) Scalability limits** \- ZooKeeper can be bottleneck. **3\) Split-brain scenarios** \- network partitions can cause issues. **KRaft (Kafka Raft)**: new consensus protocol, removes ZooKeeper dependency. **Benefits**: **1\) Simpler** \- one system instead of two. **2\) Scalable** \- better scalability, supports more partitions. **3\) Faster** \- faster metadata propagation, faster recovery. **4\) Production ready** \- GA in Kafka 3.3+. **How KRaft works**: Kafka brokers use Raft consensus algorithm for metadata management. Controller quorum (dedicated controllers) manages metadata, brokers are stateless. **Migration**: Kafka moving away from ZooKeeper, KRaft is future. New deployments should use KRaft. Understanding: ZooKeeper \= legacy, KRaft \= modern approach.

**\#. What are Kafka Streams and how do they differ from consumers?**

**Kafka Streams**: client library for building stream processing applications on top of Kafka. **Consumer**: reads messages from Kafka, processes in application code. **Kafka Streams**: higher-level abstraction for stream processing (transformations, aggregations, joins) directly on Kafka data. **Features**: **1\) Stateless operations** \- map, filter, flatMap. **2\) Stateful operations** \- aggregations, joins, windowing. **3\) State stores** \- local RocksDB for state. **4\) Exactly-once semantics** \- with transactions. **5\) Fault-tolerant** \- state backed up to Kafka. **Example \- Consumer approach**: read from "orders" topic, count orders per user in map, store in database. **Streams approach**: KStream orders \= builder.stream("orders"); orders.groupByKey().count(); \- simpler, declarative. **Use Consumer when**: simple read-process-write, need full control, integrate with external systems. **Use Kafka Streams when**: complex stream processing (joins, aggregations, windowing), stateful operations, Kafka-to-Kafka processing. **Alternatives**: Apache Flink, Spark Streaming for more complex processing. Kafka Streams \= lightweight, embedded stream processing library.

## Kafka Advanced Concepts

**\#. What are Kafka Connect and its use cases?**

Kafka Connect: framework for connecting Kafka with external systems (databases, key-value stores, search indexes, file systems). **Purpose**: import data from sources into Kafka, export data from Kafka to sinks. No coding required \- configuration-based. **Components**: **1\) Connectors** \- plugins for specific systems. **2\) Source Connectors** \- import data into Kafka (JDBC source, MongoDB source). **3\) Sink Connectors** \- export data from Kafka (Elasticsearch sink, S3 sink). **4\) Workers** \- runtime for connectors (standalone or distributed). **5\) Converters** \- data format conversion (JSON, Avro). **Example \- Database to Kafka**: JDBC Source Connector reads database (CDC), writes to Kafka topics. Changes captured in real-time. **Example \- Kafka to Elasticsearch**: Elasticsearch Sink Connector reads from Kafka, indexes in Elasticsearch. **Use cases**: **1\) Database replication**. **2\) Log aggregation**. **3\) Data warehousing** (stream to Redshift). **4\) Search indexing** (to Elasticsearch). **5\) Microservices integration**. **Benefits**: pre-built connectors, scalable, fault-tolerant, no coding. Connect makes Kafka integration hub. Hundreds of connectors available (Confluent Hub).

**\#. Explain Kafka message delivery semantics.**

Three delivery semantics: **At-Most-Once**: message delivered zero or one time, may be lost, never duplicated. **At-Least-Once**: message delivered one or more times, never lost, may duplicate. **Exactly-Once**: message delivered exactly one time, no loss, no duplication. **At-Most-Once implementation**: Producer acks=0 (no wait), consumer commits offset before processing. **Risk**: message lost on failure. **Use when**: loss acceptable (metrics, logs). **At-Least-Once implementation**: Producer acks=all (wait for replicas), consumer commits offset after processing. **Risk**: redelivery on failure (duplicates). **Use when**: duplicates tolerable with idempotent processing. **Most common**. **Exactly-Once implementation**: Idempotent producer (enable.idempotence=true) \+ transactional reads/writes. Complex, performance impact. **Use when**: critical data (financial transactions). **Best practice**: use at-least-once with idempotent processing (idempotency keys, database constraints). True exactly-once is expensive. Most systems tolerate at-least-once with proper handling.

**\#. What is idempotent producer in Kafka?**

Idempotent producer ensures message written exactly once even with retries. Prevents duplicates from retries. **Problem**: producer sends message, broker receives but ack lost. Producer retries, message duplicated. **Solution \- Idempotent Producer**: **1\) Enable** \- enable.idempotence=true. **2\) Producer ID** \- broker assigns unique ID to producer. **3\) Sequence Number** \- producer assigns sequence number to each message. **4\) Deduplication** \- broker tracks (producerId, sequence number), rejects duplicates. **Configuration**: enable.idempotence=true (automatically sets acks=all, retries=MAX, max.in.flight.requests.per.connection=5). **Guarantees**: exactly-once delivery to partition within producer session. **Limitations**: within single producer session, within partition, doesn't handle consumer duplicates. **Use with**: transactional writes for end-to-end exactly-once. **Best practice**: always enable idempotence for production. No performance penalty, prevents common duplicate issue. Idempotent producer is foundation for exactly-once semantics.

**\#. Explain Kafka transactions and exactly-once semantics.**

Transactions enable atomic writes across multiple partitions and exactly-once processing. **Use case**: read from input topic, process, write to output topic atomically. **How it works**: **1\) Transactional Producer** \- set transactional.id. **2\) Begin Transaction** \- beginTransaction(). **3\) Send Messages** \- send to multiple topics/partitions. **4\) Commit** \- commitTransaction() (atomic commit) or abort. **5\) Transactional Consumer** \- reads only committed messages (isolation.level=read\_committed). **Example**: read from "orders", process, write to "processed-orders" and "notifications" atomically. Either all succeed or all fail. **Benefits**: **1\) Atomicity** \- all or nothing. **2\) Exactly-once** \- no duplicates, no lost messages. **3\) Zombie fencing** \- prevents old producer from writing. **Configuration**: Producer: transactional.id, enable.idempotence=true. Consumer: isolation.level=read\_committed. **Trade-offs**: increased latency, complexity, coordinator overhead. **When to use**: exactly-once semantics critical (payments, inventory). Most use cases don't need transactions \- at-least-once sufficient.

**\#. What are Kafka consumer groups and rebalancing?**

**Consumer Group**: set of consumers working together, each partition assigned to one consumer in group. Enables parallel consumption and load balancing. **Rebalancing**: reassignment of partitions to consumers when group membership changes. **Triggers**: **1\) Consumer joins** \- new consumer added. **2\) Consumer leaves** \- consumer shutdown or crashes. **3\) Consumer timeout** \- heartbeat missed. **4\) Topic changed** \- partitions added. **Process**: **1\) Stop Consumption** \- consumers stop processing. **2\) Revoke Partitions** \- assigned partitions revoked. **3\) Reassign** \- group coordinator assigns partitions (range, round-robin, sticky strategies). **4\) Resume** \- consumers start processing new assignments. **Rebalance Strategies**: **Range** (default) \- contiguous ranges per consumer. **RoundRobin** \- distribute evenly. **Sticky** \- minimize movement, preserve assignments. **Issues**: **1\) Stop-the-world** \- no processing during rebalance. **2\) Duplicate processing** \- messages reprocessed if not committed. **Incremental Cooperative Rebalancing** (newer): only affected partitions reassigned, others continue. **Best practices**: tune heartbeat intervals, handle revocation cleanly, commit offsets before rebalance.

**\#. How do you optimize Kafka producer performance?**

Producer performance tuning: **1\) Batching** \- batch.size (increase for throughput, default 16KB). linger.ms (wait before send, default 0). Larger batches \= fewer requests. **2\) Compression** \- compression.type (gzip, snappy, lz4, zstd). Reduces network bandwidth. **3\) Acks** \- acks=1 (leader only) faster than acks=all (all replicas). Trade safety for speed. **4\) In-flight Requests** \- max.in.flight.requests.per.connection (default 5). More \= higher throughput, can reorder. **5\) Buffer Memory** \- buffer.memory (default 32MB). More \= handle bursts. **6\) Async Send** \- use send() callback, don't block. **7\) Partitioning** \- distribute across partitions, avoid hotspots. **Example config**: batch.size=65536, linger.ms=10, compression.type=lz4, acks=1. **Monitoring**: record-send-rate, batch-size-avg, compression-rate-avg. **Trade-offs**: throughput vs latency vs durability. High throughput: large batches, compression, acks=1. Low latency: small batches, linger.ms=0, acks=all. **Best practice**: tune based on workload, monitor metrics, test under load.

**\#. How do you optimize Kafka consumer performance?**

Consumer performance tuning: **1\) Fetch Size** \- fetch.min.bytes, fetch.max.wait.ms \- larger fetches reduce requests. **2\) Parallelism** \- scale consumers (up to partition count), process messages concurrently. **3\) Commit Strategy** \- manual commit less frequent \= better performance (but risk duplicates). **4\) Poll Timeout** \- poll(Duration) \- tune based on processing time. **5\) Consumer Threads** \- multi-threaded processing (poll in one thread, process in others). **6\) Partition Count** \- more partitions \= more parallelism. **7\) Consumer Lag** \- monitor and scale if lagging. **Example**: fetch.min.bytes=50000, max.poll.records=1000 (more messages per poll). **Patterns**: **1\) One thread per consumer** \- simple, one consumer per partition. **2\) Thread pool** \- single consumer, thread pool processes messages. **3\) Multiple consumers** \- consumer group, each consumer handles partitions. **Monitoring**: consumer-lag, fetch-rate, records-consumed-rate. **Best practice**: scale consumers to match partitions, avoid slow processing (block consumers), commit strategically, handle rebalancing gracefully.

**\#. What are Kafka log compaction and its use cases?**

Log compaction: retention policy that keeps only latest value for each key. Useful for changelog-like data. **Normal Retention**: time-based (delete.retention.ms) or size-based \- old messages deleted. **Log Compaction** (cleanup.policy=compact): retains latest value per key, deletes old values. **How it works**: background thread scans log, identifies duplicates, removes old values for same key. Tombstone (null value) marks deletion. **Use cases**: **1\) Database Changelog** \- replicate database, keep latest state per row. **2\) Microservices State** \- cache service state, latest per entity. **3\) User Profiles** \- latest profile per user. **4\) Configuration** \- latest config per service. **Example**: "user-updates" topic, key=userId. Messages: {userId:1, name:"John"}, {userId:1, name:"John Doe"}, {userId:2, name:"Jane"}. After compaction: {userId:1, name:"John Doe"}, {userId:2, name:"Jane"}. First message deleted (old value). **Configuration**: cleanup.policy=compact, min.compaction.lag.ms (how soon to compact). **Benefits**: bounded storage, maintains complete state, supports rebuilding state from log. Essential for event sourcing and CQRS.

**\#. How do you handle Kafka message ordering?**

Kafka guarantees order within partition, not across partitions. **Guarantee**: messages in same partition delivered in order written. **Challenge**: need global ordering or per-entity ordering. **Strategies**: **1\) Single Partition** \- all messages to one partition (ordered globally). **Limitation**: no parallelism, low throughput. **Use for**: small volume, must have global order. **2\) Keyed Messages** \- same key → same partition (per-key ordering). **Example**: orderId as key, all events for order123 in same partition, ordered. **3\) Multiple Partitions** \- accept unordered across entities, ordered per entity. **Most common**. **4\) Sequence Numbers** \- include sequence number in message, consumer reorders. **Handling out-of-order**: buffer and reorder, wait for missing, timestamp-based. **Best practice**: **1\) Use keys** \- partition by entity ID. **2\) Single partition per entity** \- guaranteed order. **3\) Design for eventual consistency** \- don't rely on cross-partition order. **Example**: order events keyed by orderId, inventory events keyed by productId. Each stream ordered independently. Understanding: Kafka's parallelism via partitions means accepting per-partition ordering.

**\#. What is Kafka Schema Registry and why is it important?**

Schema Registry: centralized service for managing Avro/Protobuf/JSON schemas. Confluent addition (not Apache Kafka core). **Purpose**: **1\) Schema Evolution** \- manage schema changes. **2\) Compatibility** \- ensure backward/forward compatibility. **3\) Validation** \- producers/consumers validate against schema. **4\) Storage** \- central schema repository. **How it works**: **1\) Producer** \- fetches schema ID from registry (or registers new schema). **2\) Serialize** \- serializes message using schema, includes schema ID in message. **3\) Consumer** \- reads schema ID from message, fetches schema from registry, deserializes. **Compatibility Types**: **Backward** \- new schema reads old data (can remove fields, add optional). **Forward** \- old schema reads new data (can add fields, remove optional). **Full** \- both (change optional fields only). **None** \- no checks. **Benefits**: **1\) Efficiency** \- store schema once, reference by ID (save bandwidth). **2\) Safety** \- prevent incompatible changes. **3\) Documentation** \- schemas document data format. **4\) Versioning** \- schema versions tracked. **Example**: User schema v1 {id, name}. Add email (optional): v2 {id, name, email}. Backward compatible \- old consumers ignore email. Schema Registry essential for production Kafka with structured data.

## Kafka Operations & Best Practices

**\#. How do you monitor Kafka clusters?**

Comprehensive monitoring essential for Kafka operations. **Key Metrics**: **Broker**: **1\) UnderReplicatedPartitions** \- partitions not fully replicated (critical). **2\) OfflinePartitionsCount** \- partitions without leader (critical). **3\) ActiveControllerCount** \- should be 1 per cluster. **4\) RequestsPerSecond** \- throughput. **5\) Disk usage** \- monitor space. **Producer**: **1\) record-send-rate** \- messages sent/sec. **2\) record-error-rate** \- failed sends. **3\) request-latency-avg** \- send latency. **Consumer**: **1\) consumer-lag** \- messages behind (critical). **2\) records-consumed-rate** \- consumption rate. **3\) commit-latency-avg** \- commit time. **Tools**: **1\) JMX** \- Kafka exposes metrics via JMX. **2\) Prometheus** \- scrape JMX, visualize in Grafana. **3\) Confluent Control Center** \- commercial monitoring. **4\) Kafka Manager/CMAK** \- open source UI. **5\) Burrow** \- consumer lag monitoring. **Alerts**: under-replicated partitions \> 0, consumer lag increasing, broker down, disk \> 80%. **Best practices**: monitor consumer lag closely, track partition distribution, alert on replication issues, capacity planning based on trends.

**\#. How do you scale Kafka clusters?**

Kafka scales horizontally. **Scaling Strategies**: **1\) Add Brokers** \- increase cluster capacity. **2\) Add Partitions** \- increase parallelism. **3\) Scale Consumers** \- more consumers (up to partition count). **Process \- Add Broker**: **1\) Add Server** \- start new Kafka broker. **2\) Partition Reassignment** \- use kafka-reassign-partitions tool to move partitions to new broker. **3\) Monitor** \- watch replication. **Process \- Add Partitions**: **1\) Increase Count** \- kafka-topics \--alter \--partitions N. **2\) Note**: can't decrease partitions, plan carefully. **3\) Rebalance Consumers** \- automatic rebalance happens. **Considerations**: **1\) Data Movement** \- reassignment transfers data over network. **2\) Rack Awareness** \- distribute replicas across racks. **3\) Partition Count** \- balance parallelism vs overhead (metadata, connections). **4\) Consumer Scaling** \- more partitions allows more consumers. **Best practices**: plan partition count upfront (can't decrease), monitor broker disk/CPU, use throttling during reassignment (avoid overwhelming network), automate with tools (Cruise Control for automated rebalancing). Scale proactively before hitting limits.

**\#. How do you ensure Kafka data durability?**

Data durability: ensure data not lost during failures. **Configuration**: **1\) Replication Factor** \- set \>= 3 (tolerate 2 broker failures). **2\) min.insync.replicas** \- set to 2 (at least 2 replicas must acknowledge write). **3\) Producer acks=all** \- wait for all ISR to acknowledge. **4\) Disable unclean leader election** \- unclean.leader.election.enable=false (don't elect out-of-sync replica as leader). **5\) Log flush** \- flush logs to disk periodically (or rely on OS page cache). **Example setup**: replication.factor=3, min.insync.replicas=2, acks=all. Can lose 1 broker without data loss. **Trade-offs**: **Durability vs Throughput**: acks=all slower than acks=1. **Durability vs Availability**: min.insync.replicas=2 with replication=3, if 2 brokers down, unavailable (can't meet min ISR). **Best practices**: **Production**: replication=3, min.insync=2, acks=all. **Dev/Test**: replication=1, acks=1 (faster). **Monitor**: under-replicated partitions (indicates risk). **Backup**: enable log compaction for critical topics, periodic backups. Durability configuration prevents data loss but impacts performance.

**\#. What are Kafka quotas and why use them?**

Quotas: limits on client throughput to prevent resource monopolization. **Types**: **1\) Producer Quota** \- limit bytes/sec produced. **2\) Consumer Quota** \- limit bytes/sec consumed. **3\) Request Quota** \- limit request rate. **Why use**: **1\) Fair Sharing** \- prevent one client from consuming all bandwidth. **2\) Multi-tenancy** \- isolate tenants. **3\) Cost Control** \- limit usage. **4\) Prevent Abuse** \- protect cluster. **Configuration**: **1\) Client-level** \- quota.producer.default, quota.consumer.default. **2\) User-level** \- per user quotas. **3\) Dynamic** \- kafka-configs to set quotas dynamically. **Example**: kafka-configs \--alter \--add-config 'producer\_byte\_rate=1024000' \--entity-type clients \--entity-name clientA. ClientA limited to 1MB/sec. **Throttling**: when quota exceeded, client throttled (delayed responses), not rejected. **Monitoring**: throttle-time metric tracks throttling. **Best practices**: set defaults for protection, override for specific clients, monitor throttling, set reasonable limits (avoid impacting legitimate traffic). Quotas essential for shared Kafka clusters.

**\#. How do you backup and restore Kafka data?**

Kafka isn't designed for traditional backups (data replicated across brokers). But disaster recovery still needed. **Strategies**: **1\) Cross-Cluster Replication** \- MirrorMaker 2 replicates to another cluster (different datacenter). **2\) Log Snapshots** \- snapshot broker log segments to S3/HDFS. **3\) Connect Sinks** \- stream data to external store (S3, database). **MirrorMaker 2**: **1\) Setup** \- configure source and target clusters. **2\) Replication** \- replicates topics, consumer groups, offsets. **3\) Failover** \- switch to DR cluster if primary fails. **Backup Log Segments**: **1\) Script** \- copy log segments from broker data directory to S3. **2\) Retention** \- Kafka retention eventually deletes, backup preserves. **3\) Restore** \- copy segments back, start broker. **Considerations**: **1\) Offset Management** \- backup consumer offsets (\_\_consumer\_offsets topic). **2\) Metadata** \- backup topic configs, ACLs. **3\) Test Restores** \- verify recovery process works. **Best practices**: **Production**: MirrorMaker 2 to DR cluster, test failover. **Compliance**: backup to S3 for long-term retention. Don't rely solely on Kafka replication \- plan for full cluster failure.

**\#. What are the common Kafka performance issues and solutions?**

Common issues: **1\) Consumer Lag** \- consumers can't keep up. **Solutions**: scale consumers (more instances), optimize processing (parallel processing, batching), increase partitions. **2\) High Latency** \- slow message delivery. **Solutions**: reduce linger.ms, acks=1 (vs all), compression, tune network. **3\) Under-Replicated Partitions** \- replication behind. **Solutions**: check broker health, network bandwidth, increase replica.lag.time.max.ms, add brokers. **4\) Broker Disk Full** \- log segments fill disk. **Solutions**: decrease retention, log compaction, add disk, more brokers. **5\) Rebalancing Too Frequent** \- consumer group rebalancing often. **Solutions**: increase session.timeout.ms, decrease max.poll.interval.ms, fix slow consumers. **6\) Network Saturation** \- bandwidth maxed. **Solutions**: compression, batching, more brokers, better network. **7\) Too Many Partitions** \- overhead from metadata. **Solutions**: reduce partitions (combine topics), increase broker resources. **Monitoring**: watch consumer lag, under-replication, broker CPU/disk/network. **Best practices**: capacity planning, load testing, gradual scaling, alerting on key metrics.

**\#. How do you handle Kafka security?**

Kafka security covers authentication, authorization, encryption. **Authentication** (who): **1\) SASL/PLAIN** \- username/password (simple). **2\) SASL/SCRAM** \- salted challenge-response (better). **3\) SASL/GSSAPI (Kerberos)** \- enterprise SSO. **4\) TLS/SSL** \- certificate-based (mutual TLS). **Authorization** (what): **ACLs** \- Kafka ACLs define who can produce/consume topics. Example: kafka-acls \--add \--allow-principal User:alice \--operation Read \--topic orders. **Encryption**: **1\) Encryption in Transit** \- TLS/SSL between clients and brokers, between brokers. **2\) Encryption at Rest** \- encrypt disk (OS-level, not Kafka-specific). **Configuration**: **listeners=SASL\_SSL://...** (authentication \+ encryption). **security.protocol=SASL\_SSL**. **sasl.mechanism=SCRAM-SHA-256**. **Example**: SASL\_SSL listener with SCRAM authentication, TLS encryption, ACLs for authorization. **Best practices**: **1\) Production**: always use TLS, SASL authentication. **2\) ACLs**: principle of least privilege. **3\) Rotate Credentials** \- regular rotation. **4\) Audit**: log access. **5\) Network**: isolate Kafka network. Security essential for production Kafka.

**\#. What is Kafka MirrorMaker and when do you use it?**

MirrorMaker: tool for replicating data between Kafka clusters. **Use cases**: **1\) Disaster Recovery** \- replicate to DR datacenter. **2\) Geographic Distribution** \- aggregate from regional clusters to central. **3\) Cloud Migration** \- replicate on-prem to cloud. **4\) Hybrid Cloud** \- sync between cloud and on-prem. **Two versions**: **MirrorMaker 1** (legacy): simple consumer/producer setup. Consumes from source, produces to target. Limited features. **MirrorMaker 2** (recommended): Kafka Connect-based, more features. **MM2 features**: **1\) Topic Replication** \- mirror topics. **2\) Consumer Group Replication** \- mirror offsets (enables failover). **3\) ACL Replication** \- mirror permissions. **4\) Bidirectional Replication** \- active-active setup. **Configuration**: define source and target clusters, specify topics to replicate (whitelist/blacklist). **Example**: replicate "orders" from us-west to us-east. MM2 copies messages, consumer offsets. If us-west fails, switch consumers to us-east, resume from last offset. **Best practices**: monitor replication lag, test failover, consider data sovereignty (some data can't cross regions). MirrorMaker is key for Kafka high availability and DR.

**\#. How do you upgrade Kafka versions?**

Kafka upgrades require care to avoid downtime. **Process**: **1\) Review Release Notes** \- check breaking changes, new features. **2\) Test in Non-Prod** \- test upgrade in dev/staging. **3\) Rolling Upgrade** \- upgrade brokers one at a time (no downtime). **4\) Upgrade Clients** \- upgrade producers/consumers after brokers. **Rolling Upgrade Steps**: **1\) Upgrade Binaries** \- install new version on broker. **2\) Update Config** \- set inter.broker.protocol.version to current version (for compatibility). **3\) Restart Broker** \- one broker at a time, wait for recovery. **4\) Repeat** \- for all brokers. **5\) Enable New Features** \- after all brokers upgraded, set inter.broker.protocol.version to new version. **6\) Upgrade Clients** \- update producer/consumer libraries. **Compatibility**: Kafka maintains compatibility for 2-3 versions. Clients can be older/newer than brokers within limits. **Best practices**: **1\) One version at a time** \- don't skip versions. **2\) Backup** \- before upgrade. **3\) Monitor** \- watch metrics during upgrade. **4\) Rollback Plan** \- know how to revert. **5\) Off-Peak** \- upgrade during low traffic. Kafka designed for rolling upgrades \- zero downtime is possible.

**\#. What are Kafka best practices for production?**

Comprehensive best practices: **Architecture**: **1\) Replication Factor \>= 3** \- fault tolerance. **2\) min.insync.replicas \= 2** \- durability. **3\) Unclean leader election disabled** \- prevent data loss. **4\) Multiple Availability Zones** \- distribute brokers across AZs. **Producers**: **1\) acks=all** \- ensure durability. **2\) Idempotence enabled** \- prevent duplicates. **3\) Compression** \- reduce bandwidth. **4\) Batching** \- improve throughput. **Consumers**: **1\) Manual offset commit** \- control exactly-once. **2\) Idempotent processing** \- handle duplicates. **3\) Monitor lag** \- alert on lag increases. **4\) Graceful shutdown** \- commit offsets before exit. **Operations**: **1\) Monitoring** \- comprehensive metrics, alerting. **2\) Security** \- TLS \+ SASL, ACLs. **3\) Backup/DR** \- MirrorMaker 2 to DR cluster. **4\) Capacity Planning** \- monitor growth, scale proactively. **5\) Automation** \- automate deployment, upgrades. **Configuration**: **1\) Quotas** \- protect cluster. **2\) Log Retention** \- appropriate retention for use case. **3\) Partition Count** \- based on parallelism needs. **Development**: **1\) Schema Registry** \- for data governance. **2\) Testing** \- integration tests with Kafka. **3\) Documentation** \- document topics, schemas. Following these practices ensures reliable, performant Kafka deployment.
