# Real-world Scenarios

## System Design & Architecture Scenarios

**#. Design a URL shortener service like bit.ly. What technologies would you use and why?**

**Requirements**: Generate short URLs, redirect to original, track clicks, scale to millions of requests. **Architecture**: **Frontend**: React SPA hosted on S3 + CloudFront for global CDN. **Backend**: Spring Boot microservices (URL shortening service, analytics service). **API Gateway**: Routes requests, rate limiting, authentication. **Database**: **PostgreSQL (RDS)** for URL mappings (short_code → long_url, user_id, created_at, expiry). Index on short_code for fast lookups. **Redis/ElastiCache** for caching hot URLs (95% cache hit rate, reduce DB load). TTL matching URL expiry. **ID Generation**: Base62 encoding of auto-increment ID or distributed ID generator (Twitter Snowflake pattern). **Flow**: Create: generate unique short code → store in DB → cache → return. Redirect: check Redis → if miss, query DB → cache → redirect (301/302). **Analytics**: Kafka for click events → stream to analytics service → store in DynamoDB/Elasticsearch for queries. **Scalability**: Auto Scaling for services, read replicas for DB, Redis cluster for cache. **Considerations**: collision handling, custom short codes, expiry, malicious URL detection. **Tech Stack**: Spring Boot, PostgreSQL, Redis, Kafka, AWS (S3, CloudFront, ECS).

**#. You need to design a notification system that sends emails, SMS, and push notifications. How would you architect it?**

**Requirements**: Multiple channels (email, SMS, push), reliable delivery, high throughput, retry on failure, user preferences. **Architecture**: **Notification Service** (Spring Boot): API to send notifications. **Message Queue** (Kafka/SQS): decouples producers from consumers, ensures delivery. **Topics/Queues**: email-queue, sms-queue, push-queue. **Workers**: **Email Worker**: consumes from email-queue, sends via SendGrid/SES, retries on failure (exponential backoff). **SMS Worker**: consumes from sms-queue, sends via Twilio. **Push Worker**: consumes from push-queue, sends via FCM (Firebase Cloud Messaging) for mobile. **Database**: PostgreSQL for user preferences (email enabled, SMS opt-in), notification templates. **Redis**: cache user preferences, deduplication (don't send duplicate within X minutes). **Retry Logic**: failed messages go to retry queue with delay, max retries then dead-letter queue. **Monitoring**: CloudWatch/Prometheus for queue depth, delivery rates, failures. **Flow**: Service publishes notification → Kafka routes to appropriate topic → worker consumes → sends via provider → logs result → retries on failure. **Scalability**: horizontal scaling of workers based on queue depth. **Considerations**: rate limiting per user, priority notifications, batching for efficiency, unsubscribe handling. **Tech Stack**: Spring Boot, Kafka, PostgreSQL, Redis, SendGrid, Twilio, FCM.

**#. Design a real-time chat application. What are the key components and challenges?**

**Requirements**: Real-time messaging, group chats, message history, online status, typing indicators, file sharing. **Architecture**: **WebSocket Server** (Node.js + Socket.io or Spring Boot + WebSocket): maintains persistent connections, real-time bidirectional. **Load Balancer** (ALB with sticky sessions): route to same server for WebSocket. **Message Broker** (Kafka/Redis Pub/Sub): distribute messages across server instances. **Database**: **PostgreSQL** for users, conversations, message history. **Cassandra/DynamoDB** for high-write throughput if needed. **Redis**: online status (key = user_id, TTL for auto-offline), typing indicators (ephemeral), message caching (recent messages). **Object Storage** (S3): file uploads (images, documents). **CDN** (CloudFront): deliver media files. **Flow**: User sends message → WebSocket server receives → publish to Kafka topic (conversation_id) → all server instances subscribed consume → send to connected clients → persist to DB async. **Presence**: heartbeat every 30s updates Redis TTL, offline if TTL expires. **Challenges**: **1) Connection management** - millions of concurrent connections (use Redis to track user-to-server mapping). **2) Message ordering** - Kafka partitions by conversation_id ensure order. **3) Offline users** - store messages, send push notification, deliver on reconnect. **4) Scalability** - horizontal scaling with Redis Pub/Sub for cross-server communication. **Tech Stack**: Node.js/Spring Boot, Socket.io/WebSocket, Kafka, PostgreSQL, Redis, S3.

**#. Your application is experiencing slow database queries. Walk me through your troubleshooting and optimization process.**

**Troubleshooting steps**: **1) Identify slow queries**: Enable slow query log (MySQL) or pg_stat_statements (PostgreSQL). CloudWatch RDS insights. Logs show queries taking > 1s. **2) Analyze execution plans**: EXPLAIN ANALYZE query_here. Look for full table scans, missing indexes, inefficient joins. **3) Check indexes**: Missing indexes on WHERE, JOIN, ORDER BY columns. Use EXPLAIN to verify index usage. **4) Database metrics**: CloudWatch - CPU, IOPS, connections, disk space. High CPU suggests expensive queries. High IOPS suggests disk bottleneck. **5) Application profiling**: APM tools (New Relic, DataDog) identify N+1 queries, slow endpoints. **Optimization**: **1) Add indexes**: create index on users(email), create index on orders(user_id, created_at). Composite indexes for multiple columns. **2) Query optimization**: rewrite query (avoid SELECT *, use JOINs instead of subqueries, limit result set). **3) Connection pooling**: increase pool size, tune connection limits. **4) Caching**: Redis for hot data (user sessions, product catalog). Cache invalidation strategy. **5) Read replicas**: route read queries to replicas, reduce master load. **6) Database upgrade**: bigger instance (vertical scaling) if needed. **7) Pagination**: LIMIT/OFFSET or cursor-based for large result sets. **8) Denormalization**: for complex joins, create materialized views or denormalized tables. **9) Partitioning**: partition large tables by date/range. **Prevention**: Regular ANALYZE, monitor slow queries, index strategy, load testing. **Result**: Query time reduced from 5s to 50ms.

**#. Design a video streaming platform like Netflix. What are the key architectural decisions?**

**Requirements**: Video upload, encoding, storage, streaming, recommendation, search, user management. **Architecture**: **Upload**: API Gateway → Lambda/ECS uploads to S3 raw-videos bucket. **Encoding**: S3 event triggers MediaConvert/Elastic Transcoder → encode to multiple formats (1080p, 720p, 480p), adaptive bitrate (HLS/DASH) → store in S3 encoded-videos bucket. **CDN**: CloudFront distribution with S3 origin for low-latency global delivery. **Metadata**: DynamoDB for video metadata (title, description, thumbnail, formats), fast random access. PostgreSQL for user data, watch history. **Streaming**: Video player (web/mobile) requests m3u8 manifest → CloudFront → S3 → adaptive streaming based on bandwidth. **Recommendation**: Kafka captures watch events → stream to recommendation engine (ML service) → update user preferences → store in DynamoDB. **Search**: Elasticsearch for full-text search on titles, descriptions, tags. **User Service**: Authentication (Cognito/IAM), subscriptions, viewing history. **Analytics**: Kinesis Data Streams → process watch metrics → store in Redshift for analytics. **Architecture pattern**: Microservices (video service, user service, recommendation service) with API Gateway. **Scalability**: S3 for unlimited storage, CloudFront for global CDN, Lambda/ECS auto-scaling, DynamoDB on-demand, read replicas for PostgreSQL. **Challenges**: encoding time (parallel processing), storage costs (lifecycle policies), CDN costs, DRM, video quality adaptation. **Tech Stack**: AWS (S3, CloudFront, MediaConvert, Lambda), Spring Boot, DynamoDB, PostgreSQL, Elasticsearch, Kafka.

**#. How would you design a distributed rate limiter for an API Gateway?**

**Requirements**: Limit requests per user/IP (1000 req/hour), distributed (multiple gateway instances), accurate, low latency, configurable limits. **Algorithms**: **Token Bucket** (preferred) - tokens added at rate, request consumes token, smooth burst handling. **Sliding Window Log** - accurate but memory-intensive. **Fixed Window Counter** - simple but burst at boundaries. **Implementation - Token Bucket with Redis**: **Key**: rate_limit:{user_id}, **Value**: {tokens: N, last_refill: timestamp}. **Process**: 1) GET key from Redis. 2) Calculate tokens to add based on time since last_refill (rate * time_elapsed). 3) Add tokens (max = bucket_size). 4) If tokens >= 1, decrement and allow request. Else reject (429). 5) Update Redis atomically. **Lua Script** (atomic): local tokens = redis.call('HGET', key, 'tokens'); local last = redis.call('HGET', key, 'last_refill'); -- calculate refill, check limit, update. **Redis Cluster**: hash user_id for consistent node routing. **Configuration**: DynamoDB for limits per user/tier (free: 1000/hr, premium: 10000/hr). **Distributed**: Redis is centralized, all gateway instances read same counters. **Fallback**: if Redis unavailable, fail open (allow requests) or fail closed (reject requests) based on risk tolerance. **Alternatives**: **Nginx rate limiting**, **AWS WAF rate rules**, **API Gateway throttling**. **Headers**: return X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Reset, Retry-After. **Monitoring**: rejected requests metric, Redis latency. Distributed rate limiter protects APIs.

**#. Your microservices are experiencing cascading failures. How do you prevent and handle them?**

**Scenario**: Service A calls Service B calls Service C. Service C becomes slow/unavailable → Service B times out, threads exhausted → Service B unavailable → Service A fails. **Prevention patterns**: **1) Circuit Breaker** (Resilience4j, Hystrix): detect failures (threshold: 50% error rate in 10s), open circuit (stop calls, fail fast), half-open (test recovery), closed (normal). Returns fallback response when open. **2) Timeout**: set aggressive timeouts on all calls (5s max), fail fast, don't wait indefinitely. **3) Bulkhead**: isolate thread pools per dependency (Service B has separate pools for Service C and Service D), failure in one doesn't affect others. **4) Retry with backoff**: transient failures retry (exponential backoff, jitter), don't retry 4xx errors, max 3 retries. **5) Rate Limiting**: limit requests to downstream services, prevent overwhelming. **6) Graceful Degradation**: service returns cached data or reduced functionality instead of complete failure. **7) Health Checks**: load balancer/service mesh detects unhealthy instances, stops routing traffic. **8) Async communication**: use message queues where possible, decouples services. **9) Auto Scaling**: scale up to handle load. **Implementation**: Circuit breaker on Service B's call to Service C with fallback (return cached data or default). Timeout 3s. Bulkhead with separate thread pool. Monitor metrics (circuit state, timeout rate). **Service Mesh** (Istio): automatically handles circuit breaking, retries, timeouts. **Testing**: chaos engineering (kill instances, inject latency) to validate resilience. **Result**: Service C failure doesn't cascade, Service A/B remain available with degraded functionality.

**#. Design a distributed transaction system for an e-commerce order placement involving inventory, payment, and order services.**

**Requirements**: Order creation must reserve inventory, charge payment, create order atomically. **Challenge**: Distributed transactions (inventory service, payment service, order service have separate databases), no ACID. **Solution - Saga Pattern (Orchestration)**: **Saga Orchestrator** (Order Saga Service) coordinates. **Steps**: 1) **Order Service**: create order (status: PENDING). 2) **Orchestrator calls Inventory Service**: reserve items. Success → continue. Failure → cancel order, return error. 3) **Orchestrator calls Payment Service**: charge customer. Success → continue. Failure → compensate (release inventory), cancel order, return error. 4) **Orchestrator calls Order Service**: update order (status: CONFIRMED), trigger shipping. Success → saga complete. **Compensation**: each step has compensating transaction (release inventory, refund payment, cancel order). Executed in reverse order on failure. **Implementation**: **Saga state machine**: Orchestrator stores saga state in DB (saga_id, current_step, status). Each step executed via API call (REST) or message (Kafka). On failure, execute compensations. **Idempotency**: services handle duplicate requests (payment service checks transaction_id to prevent double charge). **Database**: each service has own DB. Orchestrator DB for saga state. **Monitoring**: saga dashboard shows ongoing/failed sagas, manual intervention for stuck sagas. **Alternative - Choreography**: services react to events (OrderCreated → Inventory reserves → InventoryReserved → Payment charges → PaymentCompleted → Order confirms). Distributed but harder to trace. **Trade-offs**: eventual consistency (brief period of inconsistency), complexity (saga logic), compensation design. Saga enables distributed transactions without 2PC.

**#. How would you migrate a monolithic application to microservices with zero downtime?**

**Strategy - Strangler Fig Pattern**: gradually extract functionality, run both in parallel, eventually retire monolith. **Process**: **1) Assessment**: identify bounded contexts (user management, orders, payments, inventory), dependencies, data ownership. **2) Prioritization**: start with loosely coupled, high-value modules (authentication service first). **3) Parallel run**: deploy microservice alongside monolith, route some traffic to it (canary). **4) Data migration**: initially shared database (anti-pattern but transitional), gradually split databases. **5) Incremental extraction**: one service at a time over months. **Steps for extracting "Order Service"**: **Phase 1 - Routing**: API Gateway routes /orders/* to monolith initially. **Phase 2 - Service creation**: build Order Service with own database, migrate order data (dual writes: monolith writes to both DBs). **Phase 3 - Read switching**: API Gateway routes GET /orders to microservice, POST still to monolith. Monitor, validate. **Phase 4 - Write switching**: route POST /orders to microservice, reads/writes now in microservice. **Phase 5 - Stop dual writes**: monolith no longer writes order data. **Phase 6 - Decommission**: remove order code from monolith. **Techniques**: **Feature flags**: control routing percentage (1% → 10% → 100%). **Backward compatibility**: microservice APIs compatible with monolith clients during transition. **Data synchronization**: CDC (Debezium) keeps monolith and microservice DBs in sync during migration. **Rollback**: immediate rollback if issues (route back to monolith). **Testing**: comprehensive testing at each phase, monitor error rates, latency. **Result**: zero downtime, gradual confidence building, reversible steps. **Timeline**: 6-18 months depending on complexity.

**#. Design a leaderboard system for a mobile game with millions of users that updates in real-time.**

**Requirements**: Real-time leaderboard (top 100 globally, friends leaderboard), millions of users, frequent score updates, low latency reads. **Architecture**: **Backend**: Spring Boot APIs. **Primary Database**: **Redis Sorted Set** (perfect for leaderboards). Key: leaderboard:global, members: user_id, score: points. **Commands**: ZADD leaderboard:global 1000 user123 (add/update score), ZREVRANGE leaderboard:global 0 99 WITHSCORES (get top 100), ZREVRANK leaderboard:global user123 (get user rank), ZSCORE leaderboard:global user123 (get user score). **O(log N)** operations, handles millions. **Score update flow**: Game client sends score → API validates → ZADD to Redis (atomic update) → return new rank. **Global leaderboard**: ZREVRANGE gets top 100 instantly. Cache in CDN for 1 minute for public pages. **Friends leaderboard**: separate sorted set per user (leaderboard:friends:user123) with friend scores. Update when score changes via pub/sub or periodic refresh. **Persistence**: periodic snapshot Redis to S3, RDB persistence. PostgreSQL stores historical leaderboards (hourly/daily/weekly snapshots for analytics). **Scalability**: Redis Cluster for horizontal scaling, partition by region/game mode. **Real-time updates**: WebSocket connection pushes rank changes to clients, or polling every 30s. **Fraud detection**: validate scores server-side, rate limit score submissions, ML anomaly detection. **Time-based leaderboards**: TTL on keys (daily leaderboard expires after 24 hours), scheduled job archives to DB. **Alternatives**: DynamoDB with GSI on score (but slower than Redis), Elasticsearch (complex). **Tech Stack**: Redis Sorted Sets (primary), PostgreSQL (historical), Spring Boot, WebSocket. Redis Sorted Sets are ideal for leaderboards.

## Performance & Optimization Scenarios

**#. Your application response time increased from 200ms to 2s. How do you diagnose and fix it?**

**Diagnosis steps**: **1) Check recent deployments**: new code release? Rollback if correlated. **2) Monitoring dashboards**: CloudWatch/Grafana - check CPU, memory, error rates, throughput. Spike in any? **3) Application logs**: search for errors, exceptions, slow logs (log4j slow query logs). **4) APM tools**: New Relic, DataDog show slow transactions, identify bottleneck (DB query, external API, etc.). **5) Database**: slow query log (queries > 1s), CloudWatch RDS metrics (CPU, connections, IOPS). Missing index? Lock contention? **6) External dependencies**: third-party API timeout? Check response times. **7) Network**: latency between services? Check service mesh metrics, VPC flow logs. **8) Cache**: Redis cache hit rate dropped? Check Redis metrics (evictions, memory). **Common causes & fixes**: **DB query**: EXPLAIN shows full table scan → add index. **N+1 queries**: 100 queries for 100 items → use JOIN or batch query. **External API**: slow third-party API (payment gateway timeout from 100ms to 5s) → add timeout (3s), implement circuit breaker, cache responses. **Memory leak**: heap dump shows memory growing → restart + fix leak. **Insufficient resources**: CPU 100% → scale horizontally (more instances) or vertically (bigger instance). **Cache invalidation bug**: cache stale → restart Redis or fix invalidation logic. **Thread pool exhaustion**: all threads blocked → increase pool size or fix blocking operations. **Example**: APM shows slow DB query (2s) → EXPLAIN reveals missing index on orders.user_id → CREATE INDEX → query now 50ms → response time back to 200ms. **Prevention**: load testing, monitoring alerts, capacity planning.

**#. How would you handle a sudden 10x spike in traffic (Black Friday scenario)?**

**Preparation (before spike)**: **1) Load testing**: simulate 10x traffic with JMeter/Gatling, identify bottlenecks. **2) Auto Scaling**: configure ASGs with aggressive scale-out policies (CPU > 50% add instances), pre-warm instances if predictable (scheduled scaling). **3) Database**: read replicas for read-heavy workloads, increase IOPS, enable Multi-AZ for RDS. Consider Aurora Serverless for auto-scaling. **4) Caching**: aggressive caching (Redis/CloudFront), warm cache before spike. Cache product catalog, user sessions, API responses. TTL appropriate for data freshness. **5) CDN**: CloudFront for static assets (images, CSS, JS), reduce origin load. **6) Rate limiting**: protect backend from overwhelming traffic, queue requests if needed. **7) Async processing**: move heavy operations to background (order processing via SQS, Lambda processes async). **8) Database connection pooling**: increase pool size, tune max connections on DB. **9) Code optimization**: review and optimize hot paths, reduce unnecessary queries. **During spike**: **1) Monitor closely**: CloudWatch dashboards, set up war room. **2) Scale proactively**: manually add instances if auto-scaling lagging. **3) Disable non-critical features**: turn off recommendation engine, analytics if needed to preserve core functionality. **4) Queue requests**: if overwhelmed, use SQS to queue orders, process when capacity available (better than dropping). **5) Incident response**: on-call engineers ready to respond. **Post-spike**: **1) Scale down**: gradually reduce instances. **2) Review**: what worked, what didn't, improve for next time. **Architecture**: Stateless services (easy to scale), distributed caching, async where possible, database read replicas. **Example**: 10x spike → auto-scaling adds instances (5 → 50), CloudFront handles 80% requests (static content), Redis cache hit rate 95%, SQS queues background jobs, DB read replicas handle read load → system handles spike successfully.

**#. Your database has 1 billion records and queries are timing out. How do you optimize?**

**Scenario**: Orders table with 1B records, query: SELECT * FROM orders WHERE user_id = 123 ORDER BY created_at DESC LIMIT 10 times out (> 30s). **Optimization strategies**: **1) Indexing**: Create composite index on (user_id, created_at DESC). Query now uses index, avoids full scan → 50ms. **2) Partitioning**: Partition by date (orders_2023, orders_2024) or hash (user_id). Queries scan relevant partition only. PostgreSQL declarative partitioning or range partitioning. **3) Archiving**: Move old data (> 2 years) to archive table/S3. Active table smaller, queries faster. **4) Query optimization**: Avoid SELECT *, specify needed columns. Use covering indexes (index contains all queried columns, no table access). EXPLAIN ANALYZE verifies index usage. **5) Pagination**: Cursor-based instead of OFFSET (OFFSET scans skipped rows). WHERE created_at < :last_seen AND user_id = 123 ORDER BY created_at DESC LIMIT 10. **6) Read replicas**: Route read queries to replicas, reduce master load. **7) Caching**: Cache frequent queries (user's recent orders) in Redis with TTL. **8) Denormalization**: Create summary tables (user_order_summary) for common aggregations. **9) Database upgrade**: Bigger instance class with more IOPS. **10) Sharding**: Shard by user_id (user 1-1M on shard1, 1M-2M on shard2). Queries hit single shard. **11) NoSQL**: Consider DynamoDB for key-value access patterns (get user orders by user_id), can handle massive scale. **Example solution**: Composite index (user_id, created_at DESC) → covering index including needed columns → query 30ms. Partition by year → queries scan current year only. Archive data > 3 years to S3 → active table 200M records → query 10ms. Result: Timeout eliminated, optimal performance.

**#. How would you debug a memory leak in a Java Spring Boot application?**

**Symptoms**: Memory usage grows over time, doesn't decrease after GC, OutOfMemoryError after hours/days. **Diagnosis steps**: **1) Confirm leak**: Monitor heap usage (CloudWatch, Prometheus JVM metrics), increases continuously despite GC. **2) Heap dump**: Capture heap dump when memory high: jmap -dump:live,format=b,file=heap.hprof <pid> or trigger from CloudWatch alarm. **3) Analyze heap dump**: Eclipse MAT (Memory Analyzer Tool) or VisualVM. Look for: **Dominator tree** (objects consuming most memory), **Leak suspects** (MAT auto-detects), **Histogram** (class instances count). **4) Identify leak**: Find objects with unexpected high retention. Common culprits: Collections (List, Map) growing unbounded, Caches not evicting, Thread-local variables not cleaned, Event listeners not unregistered, DB connections not closed. **Example leak**: MAT shows HashMap with 10M entries consuming 5GB. Heap dump → Dominator tree → ConcurrentHashMap in UserSessionCache → 10M entries (users never evicted). **Fix**: Implement cache eviction (TTL-based with Caffeine cache, max size with LRU eviction). **5) Reproduce locally**: Add heap dump on OOM (-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp), load test, trigger OOM, analyze. **6) Code review**: Review recent changes, look for collection growth, unclosed resources. **7) Profiling**: YourKit, JProfiler for live profiling, shows allocation hot spots. **Common fixes**: Use weak references where appropriate, limit cache size, close resources in try-with-resources, unregister listeners, connection pooling with limits. **Prevention**: Code reviews, load testing with memory monitoring, heap dump analysis in staging, alerts on memory trends. **Result**: Identify leak in UserSessionCache → implement TTL eviction → memory stable at 2GB.

**#. Your API is getting 500 errors intermittently. Walk through your troubleshooting process.**

**Investigation**: **1) Gather data**: How often? Specific endpoints? All users or subset? Time pattern (spike at certain times)? Error rate 1% or 50%? **2) Check logs**: Application logs (log aggregator like ELK, CloudWatch Logs Insights). Search for exceptions, stack traces. Filter by 500 status. Look for patterns (specific error message, user_id, time). **3) Monitoring metrics**: APM (New Relic, DataDog) - error rate spike? Latency increase? Which endpoints? **4) Recent changes**: Recent deployment? Config change? Infrastructure change? Rollback if correlated. **5) Infrastructure health**: Check AWS health dashboard, EC2 instance status, RDS status, Redis connectivity. **6) External dependencies**: Third-party API issues? Payment gateway, auth service? Check status pages. **7) Resource exhaustion**: CPU, memory, disk, connections at limit? Thread pool exhausted? Database connections maxed out? **8) Database**: Deadlocks? Lock timeouts? Slow queries causing timeouts? Check RDS logs. **9) Load**: Traffic spike? DDoS? Check request rate. **Common causes & resolutions**: **NullPointerException**: Missing validation → add null checks, input validation. **Timeout**: Database slow → optimize query, add index. External API timeout → add timeout config, circuit breaker. **Connection pool exhausted**: 100 connections maxed → increase pool size (HikariCP), fix connection leaks. **Memory issue**: OOM → heap dump analysis, fix memory leak. **Deadlock**: Detected in DB logs → review transaction isolation, query order. **Intermittent network**: VPC/networking issue → check security groups, NACLs, route tables. **Example**: Logs show java.net.SocketTimeoutException from PaymentService → PaymentService external API slow (10s response) → add timeout (5s), implement circuit breaker with fallback ("payment processing, will notify") → 500 errors eliminated. **Resolution**: Logs identified root cause (timeout), added timeout + circuit breaker, monitored.

**#. How would you optimize a slow frontend React application?**

**Diagnosis**: Use Chrome DevTools Performance tab, Lighthouse audit. Identify: Large bundle size? Slow API calls? Excessive re-renders? Heavy computations? **Optimizations**: **1) Code splitting**: Split bundle by route (React.lazy, Suspense), load on demand. Bundle size 2MB → 200KB initial + lazy load. **2) Memoization**: React.memo (prevent re-render if props unchanged), useMemo (cache expensive calculations), useCallback (cache functions). **3) Virtualization**: Long lists (10k items) → react-window or react-virtualized (render only visible items). **4) Lazy loading images**: Intersection Observer or react-lazy-load-image-component, load images as they enter viewport. **5) API optimization**: Reduce API calls (combine endpoints), implement pagination (don't load 10k items), cache responses (React Query with cache). **6) Bundle optimization**: Remove unused dependencies (analyze with webpack-bundle-analyzer), tree shaking, minification, compression (gzip). **7) CDN**: Host static assets (images, JS, CSS) on CloudFront, faster delivery. **8) Service Worker**: Cache assets, offline support (PWA). **9) Avoid inline functions**: In JSX, don't create functions in render (creates new function each time → re-render children). **10) Debouncing**: Search input → debounce 300ms before API call (lodash debounce). **11) Web Workers**: Heavy computation offloaded to worker thread, doesn't block UI. **12) Optimize images**: Compress images, use WebP format, responsive images (srcset). **13) Reduce re-renders**: Check React DevTools Profiler, identify unnecessary re-renders, use React.memo strategically. **Example**: Bundle 3MB → code split by route → 300KB initial. List renders 5000 items → react-window → renders 20 visible items. Search debounced → reduces API calls from 10/sec to 1/300ms. Result: load time 5s → 1.5s, smooth scrolling.

**#. Design a caching strategy for an e-commerce product catalog with millions of products.**

**Requirements**: Fast product reads, handle high traffic, fresh data (inventory changes), cost-effective. **Multi-tier caching**: **1) Browser Cache**: Cache-Control headers for static assets (images: 1 year), HTML: no-cache. **2) CDN (CloudFront)**: Cache product images, static pages (category pages). TTL 1 hour for HTML, 1 day for images. Invalidate on product update. **3) Application Cache (Redis)**: Product details (id, name, price, description), inventory count. **4) Database**: PostgreSQL with proper indexes. **Redis caching strategy**: **Key structure**: product:{id}, product_list:{category}. **Product details**: Cache individual products. TTL 1 hour. Cache-aside pattern: check Redis → if miss, query DB → cache → return. **Inventory**: Separate key inventory:{product_id}, TTL 5 minutes (frequent updates), or pub/sub for real-time updates. **Category pages**: Cache list of products. product_list:electronics with pagination. TTL 30 minutes. **Search results**: Cache popular searches. search:{query} → result IDs. TTL 15 minutes. **Cache warming**: Pre-populate cache with hot products (top 1000 products) on deployment. **Invalidation**: **On product update**: Delete product:{id}, invalidate CloudFront cache for product page. **On inventory change**: Update inventory:{id} or set short TTL. **Pub/Sub**: Redis pub/sub for real-time inventory changes → application subscribers invalidate cache. **Eviction**: Redis LRU eviction, maxmemory-policy allkeys-lru. **Monitoring**: Cache hit rate (target 95%+), eviction rate, memory usage. **Fallback**: If Redis unavailable, query DB directly (degraded but available). **Cost**: Redis ElastiCache costs < database read replicas for same throughput. **Result**: 95% cache hit rate, response time 20ms (cached) vs 200ms (DB), scales to millions of requests.

**#. Your microservice is making 100 database queries for one API request (N+1 problem). How do you fix it?**

**Scenario**: GET /orders endpoint returns orders with user details. Code: Load orders (1 query), for each order load user (100 queries for 100 orders) → 101 queries. **Detection**: APM shows 100+ queries for single request, or enable Hibernate statistics (show_sql=true), or database query log. **Solutions**: **1) Eager loading with JOIN**: Single query joining orders and users. JPQL: SELECT o FROM Order o JOIN FETCH o.user WHERE .... **2) Entity Graph (JPA)**: @EntityGraph(attributePaths = {"user"}) on repository method. **3) Batch fetching (Hibernate)**: @BatchSize(size = 10) on User entity → loads users in batches (10 queries instead of 100). **4) Data loader (GraphQL)**: Facebook DataLoader batches requests. **5) Custom query**: Write DTO projection query that joins in single query. **6) Caching**: Cache users separately, load once. Not ideal but reduces DB load. **Example fix - Spring Data JPA**: Before: @Query("SELECT o FROM Order o") → N+1 problem. After: @Query("SELECT o FROM Order o JOIN FETCH o.user") → single query with JOIN. **Verification**: Enable query logging, confirm 1 query instead of 101. **Performance**: Response time 2s → 200ms (100x faster). **Prevention**: Code review for lazy loading patterns, integration tests checking query count, use JPA @EntityGraph by default for common associations. N+1 is common performance killer, JOIN FETCH is typical solution.

**#. How would you handle a third-party API that's unreliable (frequent timeouts, 500 errors)?**

**Scenario**: Payment gateway API has 10% failure rate, timeouts, intermittent 500 errors. Can't change provider (business requirement). **Resilience patterns**: **1) Timeout**: Aggressive timeout (3s max), don't wait indefinitely. **2) Retry**: Retry transient failures (500, timeout) with exponential backoff (100ms, 200ms, 400ms, max 3 retries). Don't retry 4xx. **3) Circuit Breaker**: Resilience4j - detect repeated failures (threshold: 50% error rate over 10 requests), open circuit (fail fast for 30s), half-open (test recovery), closed (normal). Return fallback response when open. **4) Fallback**: When unavailable, return "Payment processing, you'll be notified" message instead of failing entire order. **5) Async processing**: Make payment call async (SQS queue), retry in background, notify user when complete. User doesn't wait for flaky API. **6) Idempotency**: Include idempotency key in requests to prevent duplicate charges on retries. **7) Hedging**: Send request to backup payment provider simultaneously, use first response (expensive but ensures availability). **8) Rate limiting**: Don't overwhelm API, limit concurrent requests (100/sec max). **9) Monitoring**: Track success rate, latency, circuit state, alert on degradation. **10) Communication**: Notify customer of delay, send email when payment completes. **Implementation - Resilience4j**: @CircuitBreaker(name = "payment", fallbackMethod = "paymentFallback") with configuration: failureRateThreshold=50, waitDurationInOpenState=30s. @Retry(name = "payment", maxAttempts = 3) with exponentialBackoff. @Timeout(name = "payment", duration = "3s"). **Result**: Failed orders 10% → <1% (circuit breaker + retry handles transient failures), user experience improved (async processing, fallback messaging).

**#. Design a monitoring and alerting strategy for a production microservices application.**

**Comprehensive monitoring**: **Metrics (Prometheus/CloudWatch)**: **1) RED metrics** (Rate, Errors, Duration) per service - requests/sec, error rate %, p50/p95/p99 latency. **2) USE metrics** (Utilization, Saturation, Errors) - CPU %, memory %, disk I/O, network. **3) Business metrics** - orders/min, revenue/hour, signups/day. **4) Dependency metrics** - DB connection pool, cache hit rate, external API latency. **Logs (ELK/CloudWatch Logs)**: **1) Structured logging** - JSON format with timestamp, level, service, trace_id, message. **2) Centralized** - all services log to central system. **3) Log levels** - ERROR for alerts, WARN for investigation, INFO for audit. **4) Correlation IDs** - trace requests across services. **Traces (Jaeger/X-Ray)**: **1) Distributed tracing** - see request flow across microservices. **2) Latency breakdown** - which service is slow? **3) Error propagation** - where did error originate? **Alerting**: **Critical (page on-call)**: Error rate > 5%, p99 latency > 1s, service down, database down. **Warning (Slack notification)**: Error rate > 1%, disk > 80%, memory > 80%, cache hit rate < 90%. **Thresholds**: Based on SLOs (Service Level Objectives) - availability 99.9%, latency p99 < 500ms. **Alert rules (Prometheus)**: alert: HighErrorRate, expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.05, annotations: summary: "Error rate above 5%". **Dashboards (Grafana)**: **1) Service dashboard** - per service metrics (latency, throughput, errors). **2) Infrastructure dashboard** - CPU, memory, disk across instances. **3) Business dashboard** - revenue, signups, orders. **On-call**: PagerDuty integration, escalation policy, runbooks for common issues. **Health checks**: /health endpoint per service, load balancer checks health. **Result**: Issues detected quickly, mean time to detect (MTTD) < 2 min, mean time to resolve (MTTR) < 15 min.

## Data & Database Scenarios

**#. How would you design a multi-tenant SaaS database architecture?**

**Approaches**: **1) Shared database, shared schema**: All tenants in same database and tables. tenant_id column in every table. **Pros**: Cost-effective, simple. **Cons**: Security risk (query bugs expose data), scaling difficult, noisy neighbor. **2) Shared database, separate schemas**: Each tenant gets own schema (tenant1_schema, tenant2_schema). **Pros**: Better isolation, easier backup per tenant. **Cons**: Schema management complex, limited scalability. **3) Separate database per tenant**: Each tenant has own database. **Pros**: Complete isolation, independent scaling, easy to customize. **Cons**: Expensive, operational overhead (thousands of DBs). **Hybrid approach (recommended)**: **Small tenants**: Shared database with tenant_id. **Large tenants**: Dedicated database. **Implementation - Shared with tenant_id**: **Row-level security**: Every query includes WHERE tenant_id = :tenant_id. **Application layer**: Tenant context from JWT → all queries automatically filtered. **Database views**: Create view per tenant (CREATE VIEW tenant1_orders AS SELECT * FROM orders WHERE tenant_id = 'tenant1'). **Indexes**: Composite indexes starting with tenant_id (tenant_id, created_at). **Connection pooling**: Separate pool per tenant or shared pool with tenant context. **Security**: **1) Strict validation** - never trust tenant_id from client, always from authenticated session. **2) Database-level RLS** - PostgreSQL row-level security policies. **3) Audit logging** - log all data access with tenant_id. **Migration**: Start shared, move large tenants to dedicated DBs when they grow. **Monitoring**: Per-tenant metrics (query volume, storage), detect noisy neighbors. **Example**: 1000 small tenants in shared PostgreSQL, 10 large tenants in dedicated RDS instances. Shared DB has tenant_id partitioning for performance.

**#. Your database replication is lagging behind master by 10 minutes. How do you troubleshoot and fix?**

**Impact**: Reads from replica are stale, users see old data, reports incorrect. **Investigation**: **1) Check replication status**: MySQL: SHOW SLAVE STATUS; Seconds_Behind_Master: 600 (10 minutes). PostgreSQL: SELECT * FROM pg_stat_replication; replay_lag: 10 minutes. **2) Identify bottleneck**: **Network**: Slow network between master and replica? Check bandwidth, latency. VPC peering issues? **Disk I/O**: Replica disk slow? CloudWatch IOPS, disk queue depth. SSD vs HDD? **CPU**: Replica CPU saturated? Single-threaded replication can't keep up. **Replication format**: Statement-based vs row-based (row-based faster for bulk operations). **3) Master load**: Master has high write load (10k writes/sec), replica can't keep up (single-threaded replay). **Common causes & fixes**: **High write volume on master**: Reduce writes, batch operations, use connection pooling. Scale up replica (bigger instance with more IOPS). **Single-threaded replication**: PostgreSQL parallel apply workers (increase max_parallel_workers), MySQL multi-threaded replication (slave_parallel_workers). **Disk bottleneck**: Upgrade to larger instance with provisioned IOPS, use gp3 EBS with higher throughput. **Large transactions**: Break into smaller transactions, commit more frequently. **Network latency**: Cross-region replica? Use Direct Connect or VPN with higher bandwidth. Move replica to same region. **4) Monitoring**: Set alert for replication lag > 5 min. Track replication lag over time. **Prevention**: Right-size replica (same or bigger than master), monitor replication lag, optimize master writes, use parallel replication. **Example**: Replication lag due to single-threaded apply → enable parallel workers (slave_parallel_workers=4 in MySQL) → lag reduced from 10 min to 30 sec. Upgraded replica instance from db.m5.large to db.m5.2xlarge → lag eliminated.

**#. Design a data pipeline to process and analyze 1TB of daily data.**

**Requirements**: Ingest 1TB/day from multiple sources (application logs, database changes, IoT devices), process, analyze, store for queries. **Architecture**: **1) Ingestion layer**: **Kinesis Data Streams** or **Kafka** for real-time event streaming (logs, events). **AWS Database Migration Service (DMS)** or **Debezium (CDC)** for database changes. **S3** for batch uploads (CSV files, archives). **2) Processing layer**: **Stream processing**: **Kinesis Data Analytics** or **Apache Flink** for real-time transformations (filtering, aggregations, enrichment). **Batch processing**: **AWS Glue** (serverless ETL) or **Spark on EMR** for large-scale transformations. **Lambda** for simple transformations on S3 uploads. **3) Storage layer**: **S3 Data Lake**: Raw data in S3 (partitioned by date: s3://bucket/year=2024/month=03/day=15/), cost-effective storage. **Data Warehouse**: **Redshift** for analytics queries (star schema with fact and dimension tables), columnar storage for fast queries. **NoSQL**: **DynamoDB** for operational queries (key-value lookups). **4) Query layer**: **Athena**: SQL queries on S3 data (serverless, pay per query). **Redshift**: OLAP queries for business intelligence. **5) Orchestration**: **Step Functions** or **Apache Airflow** orchestrate pipeline (ingestion → processing → loading). **6) Monitoring**: CloudWatch for pipeline health, data quality checks, alerts on failures. **Flow**: Logs → Kinesis → Flink (filter, aggregate) → S3 (partitioned) → Glue (ETL) → Redshift (load) → BI tool (Tableau, QuickSight). Database → DMS CDC → S3 → Glue → Redshift. **Optimization**: Partitioning (by date/region), compression (gzip, snappy), columnar format (Parquet, ORC), parallel processing, right-sizing resources. **Cost**: S3 storage $23/TB/month (Standard), Redshift $0.25/hour (dc2.large node), Glue $0.44/DPU-hour. **Result**: 1TB/day ingested, processed, queryable in Redshift within 1 hour, historical data in S3 queryable via Athena.

**#. How would you handle data consistency in a distributed system with eventual consistency?**

**Challenge**: Eventual consistency means temporary inconsistency, can cause issues (user sees stale cart, inventory shows available when sold out). **Strategies**: **1) Accept eventual consistency**: Most cases tolerate delay (seconds). Design UI to show "processing", "updating". **2) Conflict-free Replicated Data Types (CRDTs)**: Data structures that merge automatically (counters, sets). DynamoDB supports some CRDT operations. **3) Version vectors/clocks**: Track causality, resolve conflicts deterministically. DynamoDB uses vector clocks. **4) Last Write Wins (LWW)**: Timestamp-based conflict resolution. Simple but can lose updates (clock skew issues). **5) Read repair**: On read, detect inconsistency, update stale replicas. **6) Application-level resolution**: Present conflict to user or apply business logic (shopping cart: union of items from replicas). **7) Strong consistency for critical operations**: Use strongly consistent reads where needed (DynamoDB ConsistentRead=true, Cassandra QUORUM). Trade availability for consistency. **8) Saga pattern**: Compensating transactions for distributed workflows. **9) Event sourcing**: Store events (immutable), derive state by replaying. Eventual consistency naturally handled. **10) Idempotency**: Ensure operations can be safely retried (duplicate detection via idempotency keys). **Example - E-commerce**: **Cart**: Eventual consistency OK (user adds item → replicated async → briefly different on replicas → converges). Use CRDTs for cart (merge items from replicas). **Inventory**: Critical (overselling risk) → strong consistency reads before confirming order. **Order status**: Eventual consistency → user sees "processing" until all services updated. **Monitoring**: Track inconsistency duration, alert if exceeds threshold (> 10s). **Result**: Most operations eventual (fast, available), critical operations strong (safe), user expectations set appropriately ("Your changes are being saved").

**#. Design a backup and disaster recovery strategy for a critical production database.**

**Requirements**: RTO (Recovery Time Objective) 1 hour, RPO (Recovery Point Objective) 5 minutes, must survive region failure. **RDS Multi-AZ**: **Automatic failover**: Primary in AZ-a, standby in AZ-b, synchronous replication, automatic failover < 2 minutes. **Backups**: Automated daily snapshots (retain 7 days), transaction logs every 5 minutes (point-in-time recovery). Manual snapshots for major releases (retain indefinitely). **Cross-Region Replica**: **Read replica in different region**: us-east-1 (primary) → us-west-2 (replica), asynchronous replication (lag ~1 minute). On region failure, promote replica to primary (manual, ~5 minutes). **Continuous backup**: AWS Backup automates backup across services, retention policies (daily backups for 30 days, monthly for 1 year). **Testing**: **Quarterly DR drills**: Restore from snapshot to new instance, validate data integrity, test application connectivity. Practice failover to read replica. **Recovery procedures**: **Scenario 1 - Instance failure**: Multi-AZ automatic failover, minimal downtime (1-2 min). **Scenario 2 - Data corruption**: Point-in-time restore from automated backups to 5 minutes before corruption. **Scenario 3 - Region failure**: Promote us-west-2 read replica to primary, update Route 53 DNS, redirect traffic. RTO ~1 hour (manual steps + testing). **Scenario 4 - Accidental deletion**: Restore table from snapshot (if full DB) or use point-in-time recovery. **Backup validation**: Monthly restore to verify backups are valid. **Monitoring**: CloudWatch alarms for replication lag, failed backups, Automated Backup status. **Documentation**: Runbook with step-by-step recovery procedures, contacts, escalation. **Cost**: Multi-AZ doubles cost, cross-region replica adds 100% + data transfer, backups based on storage ($0.095/GB/month). **Result**: RPO 5 minutes (transaction logs), RTO 1 hour (region failover), tested quarterly, confidence in recovery.

**#. How would you migrate from a relational database to NoSQL (PostgreSQL to DynamoDB)?**

**Motivation**: Need scalability, flexible schema, lower latency for key-value access. **Assessment**: **1) Access patterns**: Identify queries (get user by ID, list user orders, search products by category). DynamoDB optimized for key-value, not complex queries/joins. **2) Data modeling**: Relational (normalized) → DynamoDB (denormalized). Design table per access pattern. **3) Feasibility**: Can access patterns be modeled in DynamoDB? Complex joins/transactions may require keeping PostgreSQL or hybrid. **DynamoDB table design**: **Single table design**: Store multiple entity types in one table with generic PK/SK. **Example**: Users: PK=USER#{userId}, SK=METADATA. Orders: PK=USER#{userId}, SK=ORDER#{orderId}. Enables querying user's orders efficiently. **GSI**: Secondary indexes for alternate access patterns (search by email: GSI PK=email). **Migration steps**: **1) Dual-write phase**: Application writes to both PostgreSQL and DynamoDB. Validate DynamoDB data matches PostgreSQL. **2) Backfill**: Migrate existing data using AWS Data Migration Service (DMS) or custom script reading PostgreSQL → writing DynamoDB. Handle schema transformation. **3) Read switch**: Gradually route reads to DynamoDB (feature flag 1% → 100%). Monitor errors, latency. **4) Write-only to PostgreSQL**: Stop writing to PostgreSQL, only DynamoDB. **5) Decommission**: Archive PostgreSQL data, shut down. **Challenges**: **1) Schema design**: DynamoDB requires understanding access patterns upfront, no flexibility for ad-hoc queries. **2) Transactions**: DynamoDB supports transactions but limited compared to PostgreSQL. **3) Query complexity**: No joins, aggregations limited. May need additional services (Elasticsearch for search). **4) Cost**: DynamoDB on-demand more expensive than small RDS, but cheaper at scale. **Hybrid approach**: Keep PostgreSQL for complex analytical queries, use DynamoDB for operational high-throughput workloads. **Example**: User profile (high read/write) → DynamoDB for speed. Complex reporting → PostgreSQL for analytics.

**#. Your application has a database connection pool exhaustion issue. How do you diagnose and resolve it?**

**Symptoms**: Application errors "Cannot get connection from pool", threads blocked waiting for connection, application slow/unresponsive. **Diagnosis**: **1) Monitor pool metrics**: HikariCP/Connection pool stats - active connections, idle connections, waiting threads. All 100 connections in use, 50 threads waiting. **2) Thread dump**: jstack <pid> or kill -3 <pid>. Look for threads in WAITING state on connection pool. **3) Application logs**: Connection timeout exceptions (org.springframework.dao.DataAccessResourceFailureException: Could not acquire connection). **4) Database**: Check active connections on DB (PostgreSQL: SELECT count(*) FROM pg_stat_activity), max_connections setting. **5) Slow queries**: Long-running queries hold connections. Check slow query log, EXPLAIN expensive queries. **Common causes & fixes**: **Connection leak**: Code not closing connections (missing try-with-resources or @Transactional). Fix: Ensure connections always closed. Spring manages with @Transactional. **Pool size too small**: 10 connections, 100 threads → increase to 50. Formula: connections = (core_count * 2) + effective_spindle_count. **Slow queries**: Queries taking 30s hold connections → optimize queries (add indexes, rewrite). **High traffic**: Request spike → increase pool size + add more instances. **Long transactions**: Transactions span multiple service calls → break into smaller transactions, use appropriate isolation level. **Database bottleneck**: DB maxed out (CPU 100%) → scale DB, optimize queries. **Implementation fixes**: Increase pool size: spring.datasource.hikari.maximum-pool-size=50 (from 10). Add connection timeout: spring.datasource.hikari.connection-timeout=30000 (30s, fail fast). Leak detection: spring.datasource.hikari.leak-detection-threshold=60000 (warns if connection held > 60s). Fix connection leaks in code (review all JDBC code for proper closing). **Monitoring**: Alert when active connections > 80% of pool, waiting threads > 10.

**#. How would you implement full-text search for an e-commerce application with millions of products?**

**Requirements**: Search products by name/description, filter by category/price/brand, faceted search, autocomplete, typo tolerance, fast (< 100ms). **Solution - Elasticsearch**: **Why**: Purpose-built for search, full-text search, aggregations (facets), fast, scales horizontally. **Architecture**: **Primary database** (PostgreSQL): source of truth for products. **Elasticsearch**: search index, denormalized product data. **Sync mechanism**: CDC (Change Data Capture) with Debezium or application dual-write → Kafka → Elasticsearch consumer updates index. **Index design**: **Mapping**: { "mappings": { "properties": { "name": {"type": "text", "analyzer": "standard"}, "description": {"type": "text"}, "category": {"type": "keyword"}, "price": {"type": "float"}, "brand": {"type": "keyword"}, "tags": {"type": "keyword"}, "created_at": {"type": "date"} }}}. **Sharding**: 5 primary shards, 1 replica (10 total shards), distribute across nodes. **Search query example**: { "query": { "multi_match": { "query": "laptop", "fields": ["name^2", "description"] }}, "filter": { "range": { "price": { "gte": 500, "lte": 1500 }}}, "aggs": { "brands": { "terms": { "field": "brand" }}}}. Returns products matching "laptop", price 500-1500, with brand facets. **Autocomplete**: Completion suggester or edge n-gram analyzer for real-time suggestions. **Typo tolerance**: Fuzzy queries (fuzziness: "AUTO"). **Relevance tuning**: Boost name over description (^2), adjust scores based on popularity, recent products. **Performance**: Query cache, filter cache, proper shard sizing (20-50GB per shard). **Monitoring**: Query latency, indexing rate, cluster health. **Alternatives**: **Amazon OpenSearch Service** (managed Elasticsearch), **Algolia** (hosted search service, simpler but expensive), **Solr** (similar to Elasticsearch). **Cost**: OpenSearch Service ~$100/month for small cluster, scales with data. **Result**: Search response < 50ms, supports complex queries, facets, autocomplete, scales to millions.

**#. Design a solution for handling large file uploads (videos, up to 10GB) in a web application.**

**Requirements**: Upload 10GB videos, resume on failure, progress tracking, secure, scalable. **Architecture**: **1) Presigned URLs (S3)**: Backend generates presigned POST URL (valid 1 hour), returns to client. Client uploads directly to S3 (no backend bottleneck). AWS signature validates authorization. **2) Multipart upload**: Break file into parts (5MB - 5GB each), upload parts in parallel, S3 reassembles. Supports resume (reupload failed parts only). **3) Client library**: AWS SDK or libraries (uppy, react-s3-uploader) handle multipart, retries, progress. **Flow**: **1) Client requests upload**: POST /videos/upload-url → backend generates presigned S3 URL (PUT to s3://bucket/videos/user123/video.mp4) → returns to client. **2) Client uploads**: JavaScript FileReader reads file, splits into 10MB chunks, initiates multipart upload (createMultipartUpload), uploads parts (uploadPart) in parallel (5 at a time), completes upload (completeMultipartUpload). Progress bar updates. **3) S3 event triggers**: S3 event notification (ObjectCreated) → SNS/SQS → Lambda/ECS worker starts video processing (transcode with MediaConvert, thumbnail generation). **4) Metadata storage**: DynamoDB stores video metadata (user_id, video_id, S3 key, status: processing/ready, thumbnail URL). **5) CDN**: CloudFront serves videos, reduces latency. **Security**: Presigned URL expires (1 hour), user authentication required to generate URL, S3 bucket policy denies public access. **Resume upload**: Client stores part ETags locally, on failure reupload only missing parts. **Progress**: Track uploaded parts, calculate percentage. **Alternatives**: **Chunked upload to backend** (complex, backend becomes bottleneck), **FTP/SFTP** (not web-friendly), **Third-party** (Vimeo, YouTube API). **Limits**: S3 multipart supports up to 5TB files. **Cost**: S3 storage $0.023/GB/month (Standard), CloudFront $0.085/GB data transfer. **Result**: Users upload 10GB videos reliably, resume on failure.

**#. How would you implement data retention and deletion policies for GDPR compliance?**

**Requirements**: User can request data deletion (Right to be Forgotten), data auto-deleted after retention period, audit trail, anonymization. **Implementation**: **1) Data inventory**: Identify all locations of personal data (databases, logs, backups, caches, analytics). **2) Retention policies**: Define per data type (user profiles: 7 years, logs: 90 days, analytics: 2 years). **3) Automated deletion**: **Scheduled jobs**: Cron/EventBridge triggers Lambda daily to delete expired data (WHERE created_at < NOW() - INTERVAL 90 days). **TTL**: DynamoDB TTL for automatic deletion, Redis TTL for cache. **Lifecycle policies**: S3 lifecycle rules (delete objects > 90 days old). **4) User deletion request**: **API endpoint**: POST /users/{id}/delete-request creates deletion request (status: pending). **Workflow**: Async job (Step Functions) orchestrates deletion across services (user-service deletes user, order-service anonymizes orders, analytics-service removes from reports, S3 deletes files). Mark user deleted in database (soft delete first, hard delete after 30 days for recovery). **5) Anonymization**: Replace PII with pseudonyms (user_id retained for analytics, name/email replaced with "DELETED_USER"). **6) Audit log**: Log all deletions (user_id, deleted_at, requester, reason) for compliance. Retain audit logs per regulation. **7) Backups**: Encrypt backups, deletion applies to backups (challenging - may need to restore, delete, re-backup or mark as deleted and filter on restore). **8) Third-party**: API calls to delete from external services (analytics, email providers). **9) Verification**: Automated tests verify all data sources cleaned, manual audit quarterly. **Challenges**: **Distributed data** (many services), **Backups** (immutable by nature), **Analytics** (aggregated data hard to remove), **Legal holds** (can't delete data under investigation). **Monitoring**: Track deletion requests (completion rate, failures), alert on stuck requests.

## Security & Incident Response Scenarios

**#. Your application was hacked and customer data was exposed. Walk through your incident response.**

**Immediate actions (first hour)**: **1) Contain**: Isolate affected systems (terminate compromised instances, revoke credentials, block IPs). Stop further data exfiltration. **2) Assess impact**: What data exposed? How many users? What attack vector? Check CloudTrail logs, application logs, database audit logs. **3) Preserve evidence**: Snapshot compromised instances, preserve logs (don't delete), document timeline. **4) Activate incident team**: Security lead, engineering, legal, PR, executive team. **Investigation (first 24 hours)**: **5) Root cause analysis**: How did attacker get in? SQL injection? Stolen credentials? Misconfigured S3 bucket? Review logs, analyze attack patterns. **6) Scope**: Full extent of breach? All affected systems identified? **7) Remediation**: Patch vulnerability (fix SQL injection, rotate all credentials, restrict S3 bucket), deploy security updates. **8) Validation**: Ensure attacker access revoked, no backdoors remain. **Communication (24-72 hours)**: **9) Internal**: Brief executives, legal, all employees. **10) External**: Notify affected users (email, website banner), regulatory bodies (GDPR requires 72-hour notification), law enforcement if criminal. **11) Public**: PR statement acknowledging breach, steps taken, user actions needed (password reset). **Recovery**: **12) Restore**: Restore systems from clean backups, verify integrity. **13) Monitor**: Enhanced monitoring for follow-up attacks. **14) User actions**: Force password resets, enable MFA, provide credit monitoring if SSN exposed. **Post-incident (weeks)**: **15) Post-mortem**: Detailed analysis, timeline, what went wrong, what went right. **16) Improvements**: Security improvements (pen testing, code review, WAF rules, employee training), update incident response plan. **17) Legal**: Work with legal on liability, potential lawsuits. **Example**: SQL injection exposed 10k user records (names, emails, hashed passwords). Contained in 1 hour, patched in 4 hours, notified users within 48 hours, implemented WAF rules.

**#. How would you secure REST APIs against common attacks (SQL injection, XSS, CSRF)?**

**SQL Injection**: **Prevention**: **1) Parameterized queries/Prepared statements**: NEVER string concatenation (SELECT * FROM users WHERE id = :id), always bind parameters. **2) ORM frameworks**: JPA/Hibernate use parameterized queries by default. **3) Input validation**: Whitelist allowed characters, reject suspicious input. **4) Least privilege**: Database user has minimal permissions. **5) WAF**: AWS WAF rules detect SQL injection patterns. **XSS (Cross-Site Scripting)**: **Prevention**: **1) Output encoding**: Encode user input when rendering (HTML entity encoding, URL encoding). **2) Content Security Policy**: CSP header restricts script sources (Content-Security-Policy: script-src 'self'). **3) HTTPOnly cookies**: Prevent JavaScript access to cookies (session hijacking). **4) Input validation**: Sanitize input, reject <script> tags. **5) Framework protection**: React escapes by default, Angular sanitizes. **CSRF (Cross-Site Request Forgery)**: **Prevention**: **1) CSRF tokens**: Include token in forms, validate on submission. **2) SameSite cookies**: Set SameSite=Lax or Strict on cookies (prevents cross-site cookie sending). **3) Custom headers**: Require X-CSRF-Token header (cross-origin requests can't set custom headers). **4) Double submit cookies**: Cookie + header, validate match. **Additional protections**: **Authentication**: OAuth 2.0, JWT with expiry. **Rate limiting**: Prevent brute force. **HTTPS only**: TLS/SSL for encryption in transit. **Input validation**: Validate all inputs (type, length, format), reject unexpected. **Error handling**: Don't expose stack traces (leak implementation details). **Security headers**: X-Content-Type-Options: nosniff, X-Frame-Options: DENY, Strict-Transport-Security. **Dependency scanning**: Snyk, OWASP Dependency-Check for vulnerable libraries. **Penetration testing**: Regular security audits. **WAF**: AWS WAF, Cloudflare protects against OWASP Top 10. **Example**: Spring Security with CSRF protection enabled, parameterized queries, React for frontend.

**#. Design an authentication and authorization system for a multi-tenant SaaS application.**

**Requirements**: Multiple tenants, users within tenants, role-based access (admin, user), API authentication, session management, SSO support. **Architecture**: **Authentication** (who you are): **OAuth 2.0 + JWT**: **1) Login**: POST /auth/login with credentials → validate → generate JWT (claims: userId, tenantId, roles, exp) → return token. **2) Subsequent requests**: Include token in Authorization: Bearer <token> → API Gateway validates (signature, expiry) → extracts tenantId, userId → passes to service. **JWT structure**: {userId: 123, tenantId: 456, roles: ['admin'], exp: 1234567890, iss: 'myapp'}. **Token types**: **Access token** (short-lived, 15 min), **Refresh token** (long-lived, 7 days, rotate on use). **SSO integration**: SAML or OAuth 2.0 with enterprise identity providers (Okta, Azure AD, Google Workspace). **Authorization** (what you can do): **RBAC (Role-Based Access Control)**: **1) Roles**: Admin (full access), Manager (read/write), User (read only). **2) Permissions**: createOrder, viewReports, deleteUser. **3) Role-Permission mapping**: Admin has all permissions, User has read permissions. **Enforcement**: **1) API Gateway**: Check if user has required permission (Lambda authorizer). **2) Service layer**: @PreAuthorize("hasRole('ADMIN')") in Spring Security. **3) Tenant isolation**: Always filter by tenantId (from JWT), prevent cross-tenant access. **Multi-tenancy**: Extract tenantId from JWT, include in all queries (WHERE tenant_id = :tenantId), row-level security in database. **Implementation**: **User Service**: manages users, roles. **Database**: users (id, tenant_id, email, password_hash), roles (id, name), user_roles (user_id, role_id), permissions (id, name), role_permissions (role_id, permission_id). **Session management**: Stateless (JWT), no server-side sessions, scales horizontally. **Logout**: Token blacklist (Redis) for logout (add token to blacklist until expiry), or short TTL makes this less needed.

**#. Your application is experiencing a DDoS attack. How do you mitigate it?**

**Detection**: Sudden traffic spike (10x normal), CloudWatch alarms for high request rate, specific IPs/user agents, unusual geographic sources. **Immediate mitigation (minutes)**: **1) AWS Shield**: Basic (free, automatic DDoS protection), Standard ($3000/month, advanced protection, DDoS response team). **2) AWS WAF**: Block malicious IPs, rate limit (1000 req/5min per IP), geo-blocking (block countries not in customer base), block user-agents. **3) CloudFront**: Absorbs traffic at edge locations, caches responses (reduces origin load), geographic restrictions. **4) Rate limiting**: API Gateway throttling (10k requests/sec), application-level rate limiting (Redis). **5) Auto Scaling**: Scale out to handle legitimate traffic increase. **6) Blacklist IPs**: Identify attacking IPs from logs, add to WAF IP set (block). **Investigation (hours)**: **7) Traffic analysis**: Identify attack patterns (targeting specific endpoint? GET flood? SYN flood?). CloudFront access logs, VPC Flow Logs. **8) Distinguish legitimate from attack**: Legitimate users affected? Adjust thresholds to minimize false positives. **9) Origin protection**: Hide origin (CloudFront-only access), use Lambda@Edge for filtering. **Advanced mitigation**: **10) Challenge-response**: CAPTCHA for suspicious requests (rate limiting triggers CAPTCHA). **11) Behavioral analysis**: ML-based anomaly detection (AWS Shield Advanced). **12) Network ACLs**: Block specific IP ranges at network level. **13) Fail2ban**: Automated IP blocking based on patterns. **Prevention**: **1) Architecture**: CloudFront + WAF in front of all public endpoints. **2) Monitoring**: Baseline traffic patterns, alerts on anomalies. **3) Rate limiting**: Always enforce rate limits. **4) Shield Advanced**: For critical applications, DDoS response team. **Example**: DDoS attack 100k req/sec targeting /api/search → enabled WAF rate limiting (1000 req/5min per IP) → blocked top 100 attacking IPs → CloudFront absorbed 90% of traffic → service remained available.

**#. How would you implement audit logging for compliance requirements?**

**Requirements**: Log all data access/modifications, immutable logs, centralized storage, long retention (7 years), queryable, tamper-proof. **What to log**: **1) Who**: userId, tenantId, IP address. **2) What**: action (CREATE, READ, UPDATE, DELETE), resource (users, orders), resource_id. **3) When**: timestamp (ISO 8601, UTC). **4) Result**: success/failure, status code. **5) Context**: request_id (correlation), changed fields (before/after values). **Implementation**: **1) Application layer**: Log at service layer (after authorization, before operation), or use AOP (Aspect-Oriented Programming) for cross-cutting logging. **Example**: @Audit annotation on methods. **2) Log format**: Structured JSON: {timestamp: "2024-03-15T10:30:00Z", userId: 123, tenantId: 456, action: "UPDATE", resource: "user", resourceId: 789, changes: {email: {old: "old@example.com", new: "new@example.com"}}, result: "success", requestId: "abc123"}. **3) Centralized logging**: CloudWatch Logs, ELK Stack, Splunk. All services send logs to central system. **4) Immutable storage**: Write-once storage (S3 with object lock, WORM compliance mode), logs can't be modified/deleted for retention period. **5) Retention**: Lifecycle policies (retain 7 years), archive to Glacier for cost savings. **6) Encryption**: Encrypt at rest (KMS), in transit (TLS). **7) Access control**: Strict IAM policies, only security team can read audit logs. **Queryable**: **1) Search**: Elasticsearch for full-text search, Athena for S3 logs (SQL queries). **2) Dashboards**: Kibana, CloudWatch Insights for visualization. **Compliance**: **8) Integrity**: Hash logs, store hashes separately to detect tampering. **9) Monitoring**: Alert on failed access attempts, unusual patterns. **10) Regular audits**: Quarterly review of logs, demonstrate compliance. **Example**: User deletes sensitive record → log {userId: 123, action: "DELETE", resource: "customer", resourceId: 456, timestamp: "...", result: "success"}.

**#. Design a secrets management solution for a microservices architecture.**

**Requirements**: Store API keys, database passwords, tokens securely, rotation, access control, audit logging. **Solution - AWS Secrets Manager**: **Storage**: Secrets encrypted with KMS, automatically rotated (RDS passwords rotated every 30 days). **Access control**: IAM policies define who can read secrets (service roles, not individuals). **Retrieval**: Services retrieve secrets on startup or via SDK. **Architecture**: **1) Store secrets**: AWS Secrets Manager or Parameter Store (encrypted parameters). Secrets Manager for automatic rotation, Parameter Store for simpler use cases. **2) IAM roles**: Each service has IAM role with permission to read only its secrets. Order-service role can read database/order-password, not database/user-password. **3) Retrieval**: Application startup: SDK call to Secrets Manager → retrieve secret → initialize connection pool. Or environment variables (injected from Secrets Manager via ECS task definition or Kubernetes secrets). **4) Rotation**: Lambda function rotates secrets (DB password), updates Secrets Manager, updates database. Services reconnect with new password. **5) Versioning**: Secrets Manager maintains versions (current, previous), allows rollback. **Best practices**: **1) Never hardcode**: No secrets in code, config files, environment variables in Dockerfile. **2) Least privilege**: Services access only needed secrets. **3) Rotation**: Regular rotation (30-90 days), automated. **4) Audit**: CloudTrail logs all secret access. **5) Encryption**: Secrets encrypted at rest (KMS), in transit (TLS). **6) Development**: Separate secrets for dev/staging/prod, different KMS keys. **Alternatives**: **HashiCorp Vault**: More features (dynamic secrets, PKI), self-hosted. **Kubernetes Secrets**: For K8s workloads, base64 encoded (not ideal for production). **Spring Cloud Config**: Centralized config server. **Example**: Order-service → IAM role order-service-role → GetSecretValue for database/order-password → initialize HikariCP.

**#. How would you implement zero-trust security for your microservices?**

**Zero-Trust principle**: "Never trust, always verify". No implicit trust based on network location. **Implementation**: **1) Mutual TLS (mTLS)**: Every service-to-service call uses mTLS (both client and server present certificates, verify identity). Service Mesh (Istio) automates this. **2) Service-to-service authentication**: JWT or mTLS for authentication. Order-service calling Payment-service includes token proving identity. **3) Authorization per request**: Not just authentication, authorize each action. Check if Order-service can call Payment-service's /charge endpoint. **4) Least privilege**: Services have minimal permissions. Order-service can call Payment-service, but not User-service. IAM roles enforce. **5) Network segmentation**: Micro-segmentation with security groups. Each service in own security group, only allowed traffic permitted. **6) Encryption everywhere**: TLS for all communication (service-to-service, service-to-database), data at rest encrypted (KMS). **7) No implicit trust**: Private network doesn't mean trusted. Verify identity of every caller. **8) Centralized policy enforcement**: Service Mesh control plane enforces policies (Istio RBAC, OPA - Open Policy Agent). **9) Observability**: Log all access (who called what), detect anomalies (unusual service calling pattern). **10) Short-lived credentials**: Rotate credentials frequently (JWT expiry 15 min), service accounts use IAM roles (temporary credentials). **11) Micro-perimeters**: Each service has its own security perimeter, not relying on network perimeter. **Architecture with Istio**: **1) Sidecar proxies** (Envoy) next to each service, intercept all traffic. **2) mTLS automatic** - proxies handle certificate exchange, encryption. **3) Authorization policies** - Istio AuthorizationPolicy defines who can call what (Order-service can POST /payment-service/charge). **4) Identity** - Istio assigns identity (SPIFFE) to each service. **Example**: Order-service calls Payment-service → mTLS verification → AuthorizationPolicy check → allowed → request proceeds.

**#. Your company needs to achieve SOC 2 compliance. What security controls would you implement?**

**SOC 2** (System and Organization Controls) focuses on five Trust Service Criteria: Security, Availability, Processing Integrity, Confidentiality, Privacy. **Security controls**: **1) Access control**: MFA for all users, RBAC for least privilege, regular access reviews, offboarding procedures (revoke access immediately). **2) Encryption**: Data at rest (S3 SSE-KMS, EBS encryption, RDS encryption), data in transit (TLS 1.2+), key management (KMS with rotation). **3) Authentication**: Strong password policies (12+ chars, complexity), SSO with SAML, OAuth 2.0 for APIs. **4) Network security**: VPC with private subnets, security groups (least privilege), WAF (protect against OWASP Top 10), DDoS protection (Shield). **5) Vulnerability management**: Regular vulnerability scans (Nessus, AWS Inspector), patch management (automated patching), dependency scanning (Snyk). **6) Incident response**: Documented IR plan, incident response team, regular drills, logging/alerting (CloudTrail, CloudWatch). **7) Monitoring**: SIEM (Security Information and Event Management) - Splunk, centralized logging, anomaly detection (GuardDuty), alerts on suspicious activity. **8) Backup and DR**: Regular backups (automated, tested), disaster recovery plan (RTO/RPO defined), tested quarterly. **9) Change management**: All changes tracked (Jira), code review, staging environment, rollback plan. **10) Vendor management**: Assess third-party vendors, contracts include security requirements, regular reviews. **Availability**: **1) Redundancy**: Multi-AZ deployment, load balancing, auto-scaling. **2) Monitoring**: Uptime monitoring, SLOs defined (99.9% uptime), incident tracking. **Confidentiality**: **1) Data classification**: Identify sensitive data (PII, PCI, PHI), encrypt/protect accordingly. **2) Access logging**: Audit logs for data access. **Privacy**: **1) GDPR compliance**: Data retention policies, right to deletion, privacy policy. **Documentation**: **1) Policies**: Security policies, incident response procedures, business continuity plan. **2) Evidence**: Screenshots, logs, audit trails for auditors. **Timeline**: 6-12 months for full compliance.

**#. How would you secure a Kubernetes cluster running microservices?**

**Kubernetes security**: **1) Network policies**: Define which pods can communicate (NetworkPolicy). Example: frontend pods can call backend pods, but not database pods directly. Default deny, explicitly allow needed traffic. **2) RBAC (Role-Based Access Control)**: Restrict access to K8s API. Developers can deploy to dev namespace, not production. Service accounts for pods with minimal permissions. **3) Pod Security Policies**: Enforce security standards (no root user, read-only filesystems, no privileged containers). Replaced by Pod Security Standards in K8s 1.25+. **4) Secrets management**: Store secrets in K8s Secrets (encrypted at rest), inject as env vars or volumes. Use external secrets (AWS Secrets Manager with External Secrets Operator). **5) Image security**: Scan images for vulnerabilities (Trivy, Clair), use minimal base images (Alpine, distroless), trusted registries (ECR, private registry), image signing (Notary, Cosign). **6) Service mesh**: Istio for mTLS between services, traffic encryption, authorization policies. **7) Ingress security**: TLS/SSL on Ingress controller, WAF for HTTP filtering, rate limiting. **8) API server**: Secure API server (disable insecure port, enable audit logging, restrict network access). **9) Node security**: Keep nodes patched, use managed node groups (EKS), restrict SSH access (bastion host), SELinux/AppArmor. **10) Monitoring**: Falco for runtime security (detect anomalies), audit logs (K8s audit logs), SIEM integration. **11) Admission controllers**: OPA Gatekeeper enforces policies (e.g., all images must come from approved registry). **12) Least privilege**: Containers don't run as root, read-only root filesystem where possible, drop capabilities. **13) Resource limits**: Set CPU/memory limits prevent resource exhaustion attacks. **Example - Security context**: securityContext: runAsNonRoot: true, readOnlyRootFilesystem: true, capabilities: drop: [ALL]. **Best practices**: Regular security audits (kube-bench), CIS benchmarks, pod security standards, network policies, secrets encryption, RBAC.

**#. Design a fraud detection system for payment transactions.**

**Requirements**: Detect fraudulent transactions in real-time, minimize false positives, learn from patterns, scalable. **Architecture**: **1) Real-time processing**: Transaction → Kafka → Fraud Detection Service → Approve/Reject → Response (< 100ms). **2) Rule-based engine**: Initial filtering with simple rules. **Rules**: Transaction amount > $10k (flag for review), multiple transactions in short time (10 transactions in 5 min), unusual location (IP from different country than billing address), velocity checks (card used at multiple merchants quickly). **3) Machine learning model**: Train model on historical transactions (features: amount, merchant, time, location, user history). Algorithms: Random Forest, Gradient Boosting, Neural Networks. Predict fraud probability (0-1). Threshold 0.8 = high risk. **4) Feature engineering**: Transaction amount, time of day, day of week, user's average transaction amount, time since last transaction, merchant category, device fingerprint, IP address, shipping vs billing address match. **5) Model serving**: SageMaker endpoint or custom model server, real-time prediction. **6) Scoring**: Each transaction scored (0-100), > 80 = reject, 50-80 = manual review, < 50 = approve. **7) Manual review queue**: Flagged transactions go to fraud analysts via dashboard. **8) Feedback loop**: Analyst decisions (fraud/not fraud) fed back to model, retrain monthly with new data. **9) Adaptive learning**: Online learning for real-time model updates, detect new fraud patterns. **Database**: **1) Transaction history**: DynamoDB for fast lookup (user transaction history, merchant history). **2) User profiles**: Profile with normal behavior (average amount, usual merchants). **3) Fraud cases**: PostgreSQL for fraud analyst review, labeled data for training. **External APIs**: **1) IP geolocation**: Determine transaction location. **2) Device fingerprinting**: Identify device, detect account takeover. **3) 3D Secure**: Additional authentication for high-risk transactions.

**#. How to optimize api response.**

**First, I'd implement caching** - both at application level using Redis for frequently accessed data, and HTTP-level caching with proper Cache-Control headers and ETags for client-side caching.

**Second, database optimization** - this is often the biggest bottleneck. I'd fix **N+1** query problems using **JOIN FETCH**, add proper **indexes** on frequently queried columns, use pagination instead of fetching all records, and implement **database connection pooling** with HikariCP. Use **Pagination**.

**Third, payload optimization** - enable **GZIP compression** which typically reduces response size by 70-80%, **return only necessary fields using DTOs or projections** instead of full entities, and consider using GraphQL if clients need different data structures.

**Fourth, for heavy operations**, I'd make them **asynchronous** - return immediately with a job ID and let the client poll for results, or use CompletableFuture for parallel processing of independent operations.

**Finally, monitoring** - I'd use **APM** tools like **New Relic** or **DataDog** to identify slow endpoints, log response times, and set up alerts for APIs taking longer than acceptable thresholds, typically 300-500ms.
